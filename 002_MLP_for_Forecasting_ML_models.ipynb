{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## **Multi-Linear Perceptron model for Forecasting ML Models**\n","In this page, the MLP model is trained and tested for stream temperatura prediction considering the air-temperature (Ta), wind (Wind), solar radiation (SR), relative humidity (HR), the day in the year (DY), streamflow (Flow), precipitation (pp), and the shade factor (FS) as predictor variables. The daily stream temperature (Tw) is set as the only response variable."],"metadata":{"id":"_D1gJLR_keDq"}},{"cell_type":"markdown","source":["Clean variables"],"metadata":{"id":"PjF14qEDQmP8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhJGKnYBkdNI"},"outputs":[],"source":["### Clean variables\n","from IPython import get_ipython\n","get_ipython().magic('reset -sf')"]},{"cell_type":"markdown","source":["### **Reading of data, process and separation in train and test**"],"metadata":{"id":"g5MihNa1QqKR"}},{"cell_type":"code","source":["### Read Data in CSV format for temporal analysis\n","from pandas import read_csv\n","import numpy as np\n","\n","# Dir:  D:\\research\\ML_model\\new_data\n","raw_d = read_csv('006_sb31_8v_norm_2012_2018.csv', header=0, index_col=0)  # For SB 31\n","#raw_d = read_csv('006_sb59_8v_norm_2006_2012.csv', header=0, index_col=0) # For SB 59\n","\n","### Removing/Dropping no-needed variables\n","data = raw_d.drop(['scenario'],axis=1)\n","var_tested = list(data.columns.values)\n","print (\"Variables to be tested:\", list(data.columns.values)) # Print headers\n","\n","### convert df pandas to np-array\n","data = np.asarray(data, dtype=np.float32) # Convert pandas df to np array\n","\n","### split into Predictor and Response variables\n","X = data[:, :-1]\n","y = data[:, -1]\n","print (\"Data dimension (X), (y):\", X.shape, y.shape)\n","\n","# Split into train and test (Random split)\n","from sklearn.model_selection import train_test_split\n","# Test-size: 30%, Train-size: 70%\n","train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1)\n","print(\"Training and test data dimensions: \\n train_x, train_y, test_x, test_y: \\n\",\n","      train_x.shape, test_x.shape, train_y.shape, test_y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r4_EUQFkkpkS","executionInfo":{"status":"ok","timestamp":1650282411690,"user_tz":420,"elapsed":1162,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"}},"outputId":"22307a35-7bab-4687-eb21-a0e6d16c9f03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Variables to be tested: ['Ta', 'Wind', 'SR', 'HR', 'DY', 'Flow', 'pp', 'SF', 'Tw']\n","Data dimension (X), (y): (2511, 8) (2511,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (1757, 8) (754, 8) (1757,) (754,)\n"]}]},{"cell_type":"markdown","source":["### **Tuning of Hyperparameters**\n","Eleven parameters were tuned:  Activation function,kernel_initializer (initial_weigths), optimizer, learning_rate, n_epochs, batch_size, number of neurons,first set of hidden layers (layers1), second set of hidden layers (layers2), nomalization, and dropout.\n","\n","Several iterations were performed to find the best set of hyperparameters.\n","\n","Running this part can take several hours (more than 30 hours in some occasions)\n","\n","Results are displayed in a table for a better comparison of the performance of hyperparameters."],"metadata":{"id":"kQI2_wVuRJki"}},{"cell_type":"markdown","source":["#### Main libraries"],"metadata":{"id":"_nZ84pOQK6Vk"}},{"cell_type":"code","source":["import random\n","from numpy import mean, std\n","from math import sqrt\n","from keras.layers import Dense, BatchNormalization, Dropout\n","from keras.models import Sequential\n","from keras.callbacks import EarlyStopping\n","from sklearn.metrics import mean_squared_error\n","from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n"],"metadata":{"id":"bNi9Snt6ktNs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Search algorithm"],"metadata":{"id":"v7bdI4BJLEE5"}},{"cell_type":"code","source":["ids = []\n","act_all, init_w_all, opt_all, l_rate_all, n_epochs_all, batch_size_all = [],[],[],[],[],[]\n","n_neur_all, layers1_all, layers2_all =[],[],[]\n","normaliz_d_all, drop_d_all, drop_rate_all = [],[],[]\n","perfm1, perfm2 = [],[]\n","\n","n_sim = 10 #   <----------------------------------- SET number of simulation\n","\n","for i in range(n_sim):\n","  print (\"Set # \"+str(i+1)+\" in process .................\")\n","  ### Set of parameters -------------------------------------------\n","  ### Parameters to be Tuned\n","  ### Activation,init_w,optimizer,learning_rate,batch_size,n_epochs,\n","  ### n_neur,layers1, layers2, nomaliz, dropout\n","\n","  ### Hyperparameters\n","  # 1. Activation selection\n","  activat_n = ['relu','sigmoid','softplus','softsign','tanh','selu','elu','exponential','LeakyReLU']\n","  #activat_n = ['relu']\n","  activat_id = random.randint(0, len(activat_n)-1)\n","  activat_s = activat_n[activat_id]\n","\n","  # 2. kernel_initializer (itial_weigths)\n","  init_w = ['normal','uniform','zeros']\n","  #init_w = ['uniform']\n","  init_w_id = random.randint(0, len(init_w)-1)\n","  init_w_s = init_w[init_w_id]\n","\n","  # 3 & 4. Optimizer and learning-rate selection\n","\n","  l_rate = random.randint(1,200)/1000 # 0.005 #\n","  opt1 = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']\n","  #opt1 = ['Adagrad']\n","  opt_id = random.randint(0, len(opt1)-1)\n","  opt2= {'Adam':Adam(learning_rate=l_rate), 'SGD':SGD(learning_rate=l_rate),\n","                  'RMSprop':RMSprop(learning_rate=l_rate), 'Adadelta':Adadelta(learning_rate=l_rate),\n","                  'Adagrad':Adagrad(learning_rate=l_rate), 'Adamax':Adamax(learning_rate=l_rate),\n","                  'Nadam':Nadam(learning_rate=l_rate), 'Ftrl':Ftrl(learning_rate=l_rate)}\n","\n","  #print (\"opt_id:\",opt_id, \";   Optmzr:\", opt2[opt1[opt_id]].__dict__[\"_name\"])\n","\n","  # 5 & 6. n_epochs, batch_size,\n","  n_epochs = random.randint(100, 1500) # 1500 #\n","  batch_size = random.randint(100, 300) # 100 #\n","\n","  # 7. number of neurons\n","  n_neur = random.randint(100, 300) # 250 #\n","\n","  # 8 & 9 number of layers\n","  layers1 = random.randint(0, 3) # 1 #\n","  layers2 = random.randint(0, 3) # 1 #\n","\n","  # 10. nomalization\n","  normaliz_d = random.randint(0, 1) # 1 #\n","\n","  # 11. dropout\n","  drop_d = random.randint(0, 1) # 1 #\n","  drop_rate = random.randint(50, 80)/100 # 0.5 - 0.8 good values\n","  ### End set of parameters -----------------------------\n","\n","\n","  #params = [activationL, init_weight, optimizerL, n_epochs, batch_size, learning_rate]\n","  n_input = len(train_x[0]) # Number of input variables\n","  model = Sequential()\n","  ### Hidden layer 0\n","  model.add(Dense(n_neur, activation = activat_s, input_dim=n_input, kernel_initializer =init_w_s))# 'uniform'))\n","\n","  if normaliz_d > 0.5:\n","    model.add(BatchNormalization())\n","\n","  ### Hidden layer 1\n","  for j in range(layers1):\n","    model.add(Dense(n_neur, activation = activat_s))\n","    #model.add(Dense(n_neur, activation = activat_s))\n","    #model.add(Dense(n_neur, activation = activat_s))\n","\n","  if drop_d > 0.5:\n","    model.add(Dropout(drop_rate, seed=123))\n","\n","  ### Hidden layer 2\n","  for j in range(layers2):\n","    model.add(Dense(n_neur, activation = activat_s))\n","    #model.add(Dense(n_neur, activation = activat_s))\n","\n","  ### Output layer\n","  model.add(Dense(1)) #model.add(Dense(1, activation='sigmoid'))\n","  #model.compile(loss='mse', optimizer=params[2]) # Compiling the model\n","  #model.compile(loss='mse', optimizer=Adam(learning_rate = l_rate)) # 0.8279\n","  model.compile(loss='mse', optimizer = opt2[opt1[opt_id]]) # 0.8279\n","\n","  ### Fit the ML-Perceptron model\n","\n","  # More about 'EarlyStopping': https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n","  # \"batch_size\" must be > 1 and < n_samples. Ref: https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n","  early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1, mode='auto') #We will wait 'patience=10' epochs before training is stopped\n","  hist_model = model.fit(train_x, train_y,validation_data=(test_x, test_y),\n","                        epochs=n_epochs, batch_size=batch_size, verbose=0, callbacks=[early_stop])\n","  #print(hist_model.history['loss']) # Get list of 'loss' at each epoch\n","  #all_loss.append(hist_model.history['loss']) # For other error reports see: https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n","  #all_val_loss.append(hist_model.history['val_loss']) # This loss is from the 'test' data\n","\n","  #print (i+1)\n","  print (activat_id, activat_n[activat_id])\n","  print (init_w_id, init_w[init_w_id])\n","  print (opt_id, opt2[opt1[opt_id]].__dict__[\"_name\"])\n","  print (\"learning_rate:\", l_rate)\n","  print (\"n_epochs, batch_size:\", n_epochs, batch_size)\n","  print (\"neurons, L1, L2: \", n_neur, layers1, layers2)\n","  #print (\"nomalztn:\", normaliz_d)\n","  print (\"nomalztn:\", (\"Yes\" if normaliz_d else \"No\"))\n","  #print (\"Dropout:\", drop_d, drop_rate)\n","  print (\"Dropout:\", (\"Yes\" if drop_d else \"No\"), \" \", (drop_rate if drop_d else \"\"))\n","  print (\"Performance\", round(hist_model.history['loss'][-1],4),\" \", round(hist_model.history['val_loss'][-1],4))\n","  print ()\n","\n","  ids.append(str(i+1))\n","  act_all.append(activat_n[activat_id])\n","  init_w_all.append(init_w[init_w_id])\n","  opt_all.append(opt2[opt1[opt_id]].__dict__[\"_name\"])\n","  l_rate_all.append(l_rate)\n","  n_epochs_all.append(n_epochs)\n","  batch_size_all.append(batch_size)\n","  n_neur_all.append(n_neur)\n","  layers1_all.append(layers1)\n","  layers2_all.append(layers2)\n","  normaliz_d_all.append(\"Yes\" if normaliz_d else \"No\")\n","  drop_d_all.append(\"Yes\" if drop_d else \"No\")\n","  drop_rate_all.append(drop_rate)\n","  perfm1.append(round(hist_model.history['loss'][-1],3))\n","  perfm2.append(round(hist_model.history['val_loss'][-1],3))\n"],"metadata":{"id":"Zj8vbiDwm0kj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Results of the search"],"metadata":{"id":"bc4nqZhuLKzy"}},{"cell_type":"code","source":["### Print results of tuning\n","headers = ['ID', 'Train_err','Test_err','Activat','weights', 'optimz','learning_r','Epochs','Batch',\n","           'Neurons','Layers1','Layers2','Normalzn','Dropout','Drop_r']\n","scores = [ids, perfm1, perfm2, act_all, init_w_all, opt_all, l_rate_all,\n","      n_epochs_all, batch_size_all, n_neur_all, layers1_all,\n","      layers2_all, normaliz_d_all, drop_d_all, drop_rate_all\n","      ]\n","scores_t = list(map(list, zip(*scores)))\n","#print (scores_t)\n","\n","from tabulate import tabulate\n","print(tabulate(scores_t, headers = headers, tablefmt = 'orgtbl'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eb0CvdcWJQqw","executionInfo":{"status":"ok","timestamp":1650310087837,"user_tz":420,"elapsed":138,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"}},"outputId":"42d13b77-1786-4f6c-e53e-9b39a2df75d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["|   ID |   Train_err |   Test_err | Activat     | weights   | optimz   |   learning_r |   Epochs |   Batch |   Neurons |   Layers1 |   Layers2 | Normalzn   | Dropout   |   Drop_r |\n","|------+-------------+------------+-------------+-----------+----------+--------------+----------+---------+-----------+-----------+-----------+------------+-----------+----------|\n","|    1 |       2.33  |      2.512 | softsign    | normal    | Adagrad  |        0.005 |      580 |     177 |       211 |         0 |         1 | Yes        | Yes       |     0.79 |\n","|    2 |       1.375 |      1.229 | elu         | zeros     | RMSprop  |        0.005 |      720 |     196 |       150 |         0 |         1 | No         | Yes       |     0.69 |\n","|    3 |       1.473 |      0.905 | softplus    | uniform   | RMSprop  |        0.005 |     7230 |     204 |       180 |         1 |         3 | No         | Yes       |     0.65 |\n","|    4 |       4.489 |      7.942 | elu         | zeros     | RMSprop  |        0.005 |      452 |     222 |       165 |         2 |         3 | No         | No        |     0.73 |\n","|    5 |       2.29  |      2.145 | softsign    | normal    | Ftrl     |        0.005 |      966 |     122 |       195 |         2 |         1 | No         | Yes       |     0.71 |\n","|    6 |       1.217 |      1.08  | LeakyReLU   | normal    | Adagrad  |        0.005 |     1222 |     160 |       245 |         0 |         1 | Yes        | No        |     0.72 |\n","|    7 |      15.738 |     15.289 | sigmoid     | zeros     | Adadelta |        0.005 |      366 |     182 |       172 |         0 |         1 | No         | Yes       |     0.76 |\n","|    8 |       0.444 |      0.534 | LeakyReLU   | uniform   | Adamax   |        0.005 |     1450 |      98 |       250 |         0 |         2 | No         | No        |     0.7  |\n","|    9 |       0.901 |      4.761 | relu        | normal    | RMSprop  |        0.005 |     1220 |     114 |       241 |         1 |         3 | No         | Yes       |     0.76 |\n","|   10 |       0.663 |      0.809 | softsign    | uniform   | SGD      |        0.005 |     1388 |     106 |       251 |         2 |         1 | Yes        | No        |     0.8  |\n","|   11 |       0.811 |      1.082 | tanh        | uniform   | Ftrl     |        0.056 |      451 |     166 |       128 |         2 |         2 | Yes        | No        |     0.79 |\n","|   12 |       2.385 |      1.183 | softsign    | normal    | Adam     |        0.144 |      393 |     158 |       149 |         1 |         0 | No         | No        |     0.65 |\n","|   13 |      11.695 |      9.827 | LeakyReLU   | normal    | Ftrl     |        0.137 |      377 |     144 |       202 |         2 |         2 | No         | No        |     0.53 |\n","|   14 |       0.581 |      0.521 | selu        | zeros     | Adam     |        0.032 |      196 |     256 |       155 |         0 |         1 | No         | No        |     0.56 |\n","|   15 |       1.571 |      4.611 | sigmoid     | zeros     | RMSprop  |        0.028 |      218 |     149 |       188 |         1 |         1 | Yes        | No        |     0.66 |\n","|   16 |       1.219 |      1.199 | elu         | normal    | Adadelta |        0.056 |      489 |     254 |       150 |         3 |         2 | Yes        | Yes       |     0.66 |\n","|   17 |       2.171 |      1.995 | tanh        | zeros     | Adadelta |        0.068 |      233 |     217 |       217 |         3 |         2 | No         | Yes       |     0.5  |\n","|   18 |       1.452 |      1.066 | selu        | normal    | Adam     |        0.026 |      497 |     149 |       238 |         2 |         1 | Yes        | No        |     0.68 |\n","|   19 |      15.771 |     15.602 | sigmoid     | zeros     | Adamax   |        0.197 |      346 |     251 |       222 |         2 |         2 | Yes        | No        |     0.51 |\n","|   20 |     nan     |    nan     | exponential | uniform   | SGD      |        0.139 |      456 |     119 |       165 |         0 |         2 | No         | Yes       |     0.63 |\n","|   21 |      15.727 |     15.556 | relu        | normal    | Nadam    |        0.156 |      287 |     252 |       197 |         0 |         0 | No         | No        |     0.77 |\n","|   22 |       1.766 |      1.485 | tanh        | uniform   | Adamax   |        0.18  |      123 |     172 |       219 |         3 |         1 | Yes        | Yes       |     0.6  |\n","|   23 |       5.235 |      1.425 | relu        | normal    | Ftrl     |        0.063 |      489 |     176 |       174 |         2 |         0 | No         | Yes       |     0.73 |\n","|   24 |       3.753 |      7.252 | LeakyReLU   | uniform   | Adamax   |        0.18  |      261 |     133 |       100 |         0 |         3 | Yes        | No        |     0.54 |\n","|   25 |    1622.12  |   1049.5   | LeakyReLU   | uniform   | RMSprop  |        0.069 |      498 |     217 |       174 |         2 |         3 | No         | No        |     0.53 |\n","|   26 |       0.799 |      0.822 | relu        | uniform   | Adamax   |        0.101 |      202 |     211 |       186 |         0 |         0 | No         | No        |     0.74 |\n","|   27 |     nan     |    nan     | selu        | normal    | SGD      |        0.15  |      420 |     222 |       263 |         0 |         0 | Yes        | Yes       |     0.51 |\n","|   28 |      16.952 |     14.754 | exponential | zeros     | Nadam    |        0.105 |      445 |     269 |       208 |         1 |         2 | No         | Yes       |     0.6  |\n","|   29 |     378.333 |    580.585 | selu        | zeros     | RMSprop  |        0.123 |      402 |     203 |       293 |         0 |         2 | Yes        | No        |     0.67 |\n","|   30 |       1.879 |      1.67  | softsign    | normal    | Adadelta |        0.096 |      286 |     154 |       260 |         0 |         1 | No         | Yes       |     0.51 |\n","|   31 |       0.959 |      1.031 | elu         | zeros     | Adagrad  |        0.167 |      992 |     255 |       294 |         1 |         0 | No         | No        |     0.64 |\n","|   32 |      15.741 |     15.565 | sigmoid     | normal    | SGD      |        0.066 |      148 |     128 |       270 |         0 |         2 | No         | Yes       |     0.72 |\n","|   33 |     nan     |    nan     | LeakyReLU   | zeros     | SGD      |        0.098 |     1318 |     270 |       264 |         1 |         2 | No         | Yes       |     0.58 |\n","|   34 |       0.527 |      0.56  | LeakyReLU   | normal    | Adadelta |        0.185 |      970 |     223 |       218 |         1 |         0 | No         | No        |     0.61 |\n","|   35 |      35.265 |     32.154 | exponential | uniform   | Adadelta |        0.11  |      797 |     299 |       289 |         1 |         2 | Yes        | Yes       |     0.56 |\n","|   36 |      29.215 |     27.632 | exponential | uniform   | Adadelta |        0.058 |     1037 |     177 |       297 |         3 |         0 | No         | No        |     0.67 |\n","|   37 |       1.103 |      8.618 | elu         | normal    | Adam     |        0.019 |      972 |     206 |       250 |         1 |         3 | Yes        | Yes       |     0.58 |\n","|   38 |       1.108 |      0.722 | sigmoid     | normal    | Adadelta |        0.177 |      719 |     259 |       300 |         2 |         1 | Yes        | Yes       |     0.76 |\n","|   39 |       1.382 |      1.013 | selu        | normal    | Adagrad  |        0.067 |      509 |     150 |       270 |         0 |         1 | Yes        | Yes       |     0.73 |\n","|   40 |       1.469 |      1.646 | LeakyReLU   | uniform   | Adamax   |        0.195 |     1037 |     110 |       121 |         0 |         3 | No         | No        |     0.76 |\n","|   41 |       7.785 |      4.856 | exponential | normal    | Adagrad  |        0.071 |     1346 |     251 |       139 |         3 |         3 | No         | No        |     0.6  |\n","|   42 |     nan     |    nan     | relu        | normal    | SGD      |        0.117 |      926 |     287 |       223 |         2 |         1 | No         | Yes       |     0.69 |\n","|   43 |       2.604 |      1.658 | sigmoid     | uniform   | Adadelta |        0.051 |     1466 |     260 |       267 |         2 |         0 | Yes        | Yes       |     0.51 |\n","|   44 |       6.312 |      1.097 | relu        | zeros     | Adagrad  |        0.046 |     1360 |     268 |       189 |         3 |         0 | No         | Yes       |     0.77 |\n","|   45 |      15.887 |     15.976 | tanh        | uniform   | Adam     |        0.198 |      664 |     296 |       138 |         2 |         3 | No         | Yes       |     0.71 |\n","|   46 |       3.638 |      1.385 | tanh        | normal    | Adamax   |        0.09  |      153 |     174 |       142 |         2 |         0 | Yes        | Yes       |     0.69 |\n","|   47 |      15.755 |     15.512 | sigmoid     | uniform   | Adamax   |        0.198 |      980 |     232 |       180 |         3 |         0 | No         | No        |     0.5  |\n","|   48 |      16.778 |     20.047 | sigmoid     | uniform   | RMSprop  |        0.017 |      822 |     260 |       247 |         2 |         2 | No         | No        |     0.52 |\n","|   49 |       4.483 |      3.319 | elu         | normal    | Adamax   |        0.053 |      151 |     213 |       244 |         1 |         2 | No         | No        |     0.6  |\n","|   50 |      42.514 |     39.564 | exponential | uniform   | Adam     |        0.169 |      329 |     102 |       106 |         3 |         1 | Yes        | No        |     0.62 |\n","|   51 |    9039.5   |    526.941 | LeakyReLU   | uniform   | Adamax   |        0.2   |      360 |     257 |       256 |         1 |         3 | No         | Yes       |     0.73 |\n","|   52 |     151.326 |    186.438 | softsign    | normal    | RMSprop  |        0.114 |      126 |     202 |       283 |         3 |         3 | Yes        | Yes       |     0.54 |\n","|   53 |      26.385 |     23.158 | selu        | zeros     | SGD      |        0.017 |      439 |     152 |       268 |         1 |         0 | Yes        | Yes       |     0.77 |\n","|   54 |       0.764 |      1.802 | LeakyReLU   | zeros     | Ftrl     |        0.035 |      415 |     218 |       122 |         0 |         3 | Yes        | No        |     0.71 |\n","|   55 |     136.972 |   1466.31  | tanh        | uniform   | RMSprop  |        0.156 |      379 |     193 |       231 |         1 |         2 | Yes        | No        |     0.78 |\n","|   56 |      15.731 |     15.612 | relu        | zeros     | Adam     |        0.109 |      860 |     125 |       229 |         0 |         1 | No         | Yes       |     0.58 |\n","|   57 |       1.258 |      1.523 | softplus    | zeros     | SGD      |        0.001 |      542 |     208 |       218 |         0 |         2 | Yes        | No        |     0.56 |\n","|   58 |      15.727 |     15.562 | relu        | normal    | Ftrl     |        0.193 |      720 |     181 |       184 |         2 |         0 | Yes        | Yes       |     0.77 |\n","|   59 |     694.747 |    499.551 | selu        | normal    | Nadam    |        0.176 |     1156 |     297 |       240 |         3 |         0 | Yes        | No        |     0.74 |\n","|   60 |      16.86  |     16.54  | relu        | uniform   | RMSprop  |        0.128 |     1446 |     263 |       110 |         0 |         0 | Yes        | No        |     0.5  |\n","|   61 |      15.727 |     15.552 | softplus    | normal    | Adamax   |        0.122 |      828 |     230 |       224 |         3 |         3 | No         | Yes       |     0.68 |\n","|   62 |       1.287 |      3.718 | relu        | normal    | RMSprop  |        0.01  |      532 |     224 |       207 |         1 |         3 | Yes        | Yes       |     0.57 |\n","|   63 |      90.395 |     88.604 | selu        | uniform   | Adam     |        0.104 |      109 |     158 |       200 |         2 |         3 | Yes        | No        |     0.52 |\n","|   64 |       1.637 |      1.475 | LeakyReLU   | zeros     | Nadam    |        0.035 |      218 |     173 |       191 |         0 |         3 | No         | No        |     0.64 |\n","|   65 |      52.361 |     48.245 | softplus    | uniform   | SGD      |        0.023 |      964 |     262 |       210 |         2 |         2 | No         | Yes       |     0.72 |\n","|   66 |       1.894 |      1.679 | LeakyReLU   | uniform   | Adadelta |        0.03  |      466 |     157 |       176 |         2 |         3 | No         | Yes       |     0.7  |\n","|   67 |       0.93  |      0.962 | softplus    | zeros     | Adadelta |        0.123 |     1426 |     261 |       240 |         0 |         3 | Yes        | No        |     0.7  |\n","|   68 |     128.336 |    134.45  | softplus    | zeros     | Nadam    |        0.169 |     1300 |     286 |       196 |         1 |         2 | No         | Yes       |     0.74 |\n","|   69 |     132.415 |    126.854 | elu         | zeros     | Nadam    |        0.179 |     1029 |     172 |       199 |         3 |         3 | No         | Yes       |     0.62 |\n","|   70 |    7298.94  |   7349.15  | elu         | normal    | Adam     |        0.198 |      421 |     184 |       151 |         3 |         2 | Yes        | No        |     0.52 |\n"]}]},{"cell_type":"markdown","source":["### **Modeling employing the tuned parameters (best parameters)**\n","From the previous step, we got that the set of parameters #8 provides the best performance."],"metadata":{"id":"pK7If33OLljc"}},{"cell_type":"code","source":["### Read Data in CSV format for temporal analysis\n","from pandas import read_csv\n","import numpy as np\n","\n","# Dir:  D:\\research\\ML_model\\new_data\n","#raw_d = read_csv('006_sb31_8v_norm_2012_2018.csv', header=0, index_col=0)  # For SB 31\n","raw_d = read_csv('002_sb31_norm_2012_2017_for_train_test.csv', header=0, index_col=0)  # For SB 31\n","#raw_d = read_csv('006_sb59_8v_norm_2006_2012.csv', header=0, index_col=0) # For SB 59\n","\n","### Removing/Dropping no-needed variables\n","data = raw_d.drop(['scenario'],axis=1)\n","var_tested = list(data.columns.values)\n","print (\"Variables to be tested:\", list(data.columns.values)) # Print headers\n","\n","### convert df pandas to np-array\n","data = np.asarray(data, dtype=np.float32) # Convert pandas df to np array\n","\n","### split into Predictor and Response variables\n","X = data[:, :-1]\n","y = data[:, -1]\n","print (\"Data dimension (X), (y):\", X.shape, y.shape)\n","\n","# Split into train and test (Random split)\n","from sklearn.model_selection import train_test_split\n","# Test-size: 30%, Train-size: 70%\n","train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1)\n","print(\"Training and test data dimensions: \\n train_x, train_y, test_x, test_y: \\n\",\n","      train_x.shape, test_x.shape, train_y.shape, test_y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p-dX-AAR3MUW","executionInfo":{"status":"ok","timestamp":1652196365593,"user_tz":420,"elapsed":134,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"}},"outputId":"3cb67767-9256-4a25-8c70-949dd2f75d3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Variables to be tested: ['Ta', 'Wind', 'SR', 'HR', 'DY', 'Flow', 'pp', 'SF', 'Tw']\n","Data dimension (X), (y): (6438, 8) (6438,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4506, 8) (1932, 8) (4506,) (1932,)\n"]}]},{"cell_type":"markdown","source":["#### Setting the MLP model with the best parameters.\n","This has to be manually set"],"metadata":{"id":"vxuKbKP1MDOi"}},{"cell_type":"code","source":["### MLP-Model definition with best parameters (This setting is not automatically updated)\n","def best_nn_model(n_inp):\n","  act_win = 'LeakyReLU'\n","  weight_win = 'uniform'\n","\n","  n_input = n_inp #len(train_x[0]) # Number of input variables\n","  b_model = Sequential()\n","  ### Input layer 0\n","  b_model.add(Dense(250, activation = act_win, input_dim=n_input, kernel_initializer = weight_win))\n","  #b_model.add(BatchNormalization())\n","\n","  ### Hidden layer 1\n","  #b_model.add(Dense(250, activation = act_win))\n","  #b_model.add(Dense(250, activation = act_win))\n","  #b_model.add(Dense(250, activation = act_win))\n","  #model.add(Dropout(drop_rate, seed=123))\n","\n","  ### Hidden layer 2\n","  b_model.add(Dense(250, activation = act_win))\n","  b_model.add(Dense(250, activation = act_win))\n","  #b_model.add(Dense(250, activation = act_win))\n","\n","  ### Output layer\n","  b_model.add(Dense(1))\n","\n","  #b_model.compile(loss='mse', optimizer=Adagrad(learning_rate = 0.005))\n","  b_model.compile(loss='mse', optimizer=Adamax(learning_rate = 0.005))\n","  return b_model"],"metadata":{"id":"wRkz39oF8_68"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Setting the process to repeat the model for different sets of predictors\n","20 repetitions were conducted for each set of predictors."],"metadata":{"id":"660La_g7Miy6"}},{"cell_type":"code","source":["from termcolor import colored\n","from sklearn.model_selection import train_test_split\n","\n","data = raw_d.drop(['scenario'],axis=1)\n","'''\n","set1 = data[['Ta','Tw']]\n","set2 = data[['Ta','Flow','Tw']]\n","set3 = data[['Ta','Flow','DY','Tw']]\n","set4 = data[['Ta','Flow','DY','SF','Tw']]\n","set5 = data[['Ta','Flow','DY','SF','SR','Tw']]\n","set6 = data[['Ta','Flow','DY','SF','SR','pp','Tw']]\n","set7 = data[['Ta','Flow','DY','SF','SR','pp','HR','Tw']]\n","set8 = data[['Ta','Flow','DY','SF','SR','pp','HR','Wind','Tw']]\n","set9 = data[['Ta','DY','SF','SR','pp','HR','Wind','Tw']]\n","set10 = data[['Ta','DY','pp','Tw']]\n","set11 = data[['Ta','DY','Tw']] #'''\n","\n","set1 = data[['Ta','Flow','Tw']]\n","set2 = data[['Ta','DY','Tw']]\n","set3 = data[['Ta','SF','Tw']]\n","set4 = data[['Ta','SR','Tw']]\n","set5 = data[['Ta','pp','Tw']]\n","set6 = data[['Ta','HR','Tw']]\n","set7 = data[['Ta','Wind','Tw']]\n","\n","#sets = [set1,set2,set3,set4,set5,set6,set7,set8,set9,set10,set11]\n","sets = [set1,set2,set3,set4,set5,set6,set7]\n","labels=[str(r) for r in sets]\n","sets_names = [f'M{i}' for i in range(1, 12)]\n","#print(\"Models to be tested: \", sets_names)\n","\n","names = [['Ta','Tw'],['Ta','Flow','Tw'],['Ta','Flow','DY','Tw'],['Ta','Flow','DY','SF','Tw'],['Ta','Flow','DY','SF','SR','Tw']\n","         ,['Ta','Flow','DY','SF','SR','pp','Tw'],['Ta','Flow','DY','SF','SR','pp','HR','Tw']\n","         ,['Ta','Flow','DY','SF','SR','pp','HR','Wind','Tw'],['Ta','DY','SF','SR','pp','HR','Wind','Tw']\n","         ,['Ta','DY','pp','Tw'],['Ta','DY','Tw']]\n","names = [['Ta','Flow','Tw'],['Ta','DY','Tw'],['Ta','SF','Tw'],['Ta','SR','Tw'],['Ta','pp','Tw']\n","         ,['Ta','HR','Tw'],['Ta','Wind','Tw']\n","         ]\n","\n","all_sets = []\n","ccc = 0\n","for sett in sets:\n","  #print (\"Model M\"+str(ccc+1)+\"......\", 'red')\n","  print (colored(\"Model M\"+str(ccc+1)+\"......\"+str(names[ccc][:-1]), 'red'))\n","  sett = np.asarray(sett, dtype=np.float32) # Convert pandas df to np array\n","\n","  ### split into Predictor and Response variables\n","  Xn = sett[:, :-1]\n","  yn = sett[:, -1]\n","  print (\"Data dimension (Xn), (yn):\", Xn.shape, yn.shape)\n","\n","  # Split into train and test (Random split)\n","\n","  # Test-size: 30%, Train-size: 70%\n","  train_xn, test_xn, train_yn, test_yn = train_test_split(Xn, yn, test_size=0.3, random_state=1)\n","  print(\"Training and test data dimensions: \\n train_x, train_y, test_x, test_y: \\n\",\n","        train_xn.shape, test_xn.shape, train_yn.shape, test_yn.shape)\n","  #print (\"train_data:\",train_xn)\n","  #print (\"train_y:\",train_yn)\n","\n","  n_epochs = 1500   #  <-----  you might SET\n","  batch_size = 64 #\n","\n","  all_loss, all_val_loss = [],[]\n","  all_preds, all_scores_train, all_scores_test = [], [], []\n","\n","  n_repeats = 2 #20  #     <----------------------- SET\n","  for i in range(n_repeats):\n","    print (\"Sim\", i+1)\n","    b_model = best_nn_model(len(train_xn[0]))   #  -----> Function model\n","    early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1, mode='auto') #We will wait 'patience=10' epochs before training is stopped\n","\n","    hist_model = b_model.fit(train_xn, train_yn,validation_data=(test_xn, test_yn),\n","                          epochs=n_epochs, batch_size=batch_size, verbose=0, callbacks=[early_stop])\n","    #print(hist_model.history['loss']) # Get list of 'loss' at each epoch\n","    all_loss.append(hist_model.history['loss']) # For other error reports see: https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n","    all_val_loss.append(hist_model.history['val_loss']) # This loss is from the 'test' data\n","\n","    print (\"Loss (MSE):   \", hist_model.history['loss'][-1]) # This is MSE for train\n","    print (\"Val_loss (MSE):\", hist_model.history['val_loss'][-1]) # This is MSE for test\n","\n","\n","    ### Prediction\n","    ### Prediction on train\n","    pred_yn_train = b_model.predict(train_xn, verbose=0)\n","    pred_yn_train_flat = [item for sublist in pred_yn_train.tolist() for item in sublist]\n","    #print (\"Prediction on train data (Sim\",i+1, \") \\n\", pred_yn_train_flat)\n","    all_preds.append(pred_yn_train_flat)\n","    #print (dasda)\n","    #pred_y_test_flat.to_csv('predict.csv')  # save updated dataset\n","\n","    # Prediction Error. Root mean squared error or RMSE\n","    error_train = sqrt(mean_squared_error(train_yn, pred_yn_train))#error = measure_rmse(test, predictions)\n","    #print(\"RMSE train (sim\",i+1,\"):\",'  %.3f' % error_train) #E:\n","\n","    all_scores_train.append(error_train) #all_scores.append(scores)\n","    #print (all_scores_train)\n","\n","    ### Prediction on test\n","    pred_yn_test = b_model.predict(test_xn, verbose=0)\n","    pred_yn_test_flat = [item for sublist in pred_yn_test.tolist() for item in sublist]\n","    #print (\"Prediction on test data (Sim\",i+1, \") \\n\", pred_yn_test_flat)\n","    all_preds.append(pred_yn_test_flat)\n","    #print (dasdsa)\n","    #pred_y_test_flat.to_csv('predict.csv')  # save updated dataset\n","\n","    # Prediction Error. Root mean squared error or RMSE\n","    error_test = sqrt(mean_squared_error(test_yn, pred_yn_test))#error = measure_rmse(test, predictions)\n","    #print(\"RMSE test(sim\",i+1,\"):\",'   %.3f' % error_test) #E:\n","\n","    all_scores_test.append(error_test) #all_scores.append(scores)\n","    #print (all_scores_test)\n","    #print (ewewe)\n","\n","  #print (\"Tested variables (Header):\", var_tested) # print column names\n","  # summarize and plot scores\n","  print (\"Note: loss is MSE\")\n","  scores_m, score_std = mean(all_scores_test), std(all_scores_test)\n","  print('%s: %.3f RMSE_avge (+/- %.3f)' % ('Average of '+str(n_repeats)+\" repetitions\", scores_m, score_std))\n","\n","  all_sets.append(all_scores_test) #Store scores of all models (M1, M2,...) as list od lists\n","  print ()\n","  ccc += 1\n"],"metadata":{"id":"7mQSHIe39GoU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Save output to CSV file"],"metadata":{"id":"yK45arsMPRGW"}},{"cell_type":"code","source":["### Save output data as CSV file\n","from google.colab import files\n","import pandas as pd\n","scores_df = pd.DataFrame(all_sets)\n","scores_df.to_csv('SB31_mlp_output_11models.csv')\n","#scores_all_models"],"metadata":{"id":"LCJyIBSuXIbt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Plot of Performance of the tested models with the different sets of predictors"],"metadata":{"id":"56OfVq9YO6kp"}},{"cell_type":"code","source":["### Plot of results\n","import matplotlib.pyplot as plt\n","scores_all_models = all_sets\n","#sets_names = ['M1','M2','M3','M4','M5','M6','M7','M8','M9','M10','M11']\n","### Plot performance of all the models\n","fig = plt.figure(figsize=(8, 4), dpi=80)\n","plt.boxplot(scores_all_models, labels=sets_names, showmeans=1,\n","            meanprops={\"marker\":\"s\",\"markersize\":\"4\",\"markerfacecolor\":\"white\",\n","                       \"markeredgecolor\":\"blue\", \"markeredgewidth\":\"2\"})\n","\n","plt.xlabel(\"MLP Model\")\n","plt.ylabel(\"RMSE\")\n","plt.grid()\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"8Mpnlfb-Ar_n","executionInfo":{"status":"ok","timestamp":1652162061961,"user_tz":420,"elapsed":518,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"}},"outputId":"73c56ec2-324f-4c0e-faa1-403d6f0bbc01"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x320 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":[],"metadata":{"id":"ww4Gjs7Q9Gbh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Feasible area employing the tuned parameters**"],"metadata":{"id":"7RZmJfZnQBRp"}},{"cell_type":"code","source":["import random\n","from numpy import mean, std\n","from math import sqrt\n","from keras.layers import Dense, BatchNormalization, Dropout\n","from keras.models import Sequential\n","from keras.callbacks import EarlyStopping\n","from sklearn.metrics import mean_squared_error\n","from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl"],"metadata":{"id":"qopSN3zwr0ik"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Clean variables\n","from IPython import get_ipython\n","get_ipython().magic('reset -sf')"],"metadata":{"id":"zkMf5_ttiuMf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Read Data in CSV format for temporal analysis\n","from pandas import read_csv\n","import numpy as np\n","\n","# Dir:  D:\\research\\ML_model\\new_data\n","#raw_d = read_csv('006_sb31_8v_norm_2012_2018.csv', header=0, index_col=0)  # For SB 31\n","#raw_d = read_csv('002_sb31_norm_2012_2017_for_train_test.csv', header=0, index_col=0)  # For SB 31\n","raw_d = read_csv('004_sb59_norm_2006_2011_for_train_test.csv', header=0, index_col=0)  # For SB 31\n","#raw_d = read_csv('006_sb59_8v_norm_2006_2012.csv', header=0, index_col=0) # For SB 59\n","\n","### Removing/Dropping no-needed variables\n","data = raw_d.drop(['scenario'],axis=1)\n","var_tested = list(data.columns.values)\n","print (\"Variables to be tested:\", list(data.columns.values)) # Print headers\n","\n","### convert df pandas to np-array\n","data = np.asarray(data, dtype=np.float32) # Convert pandas df to np array\n","\n","### split into Predictor and Response variables\n","X = data[:, :-1]\n","y = data[:, -1]\n","print (\"Data dimension (X), (y):\", X.shape, y.shape)\n","\n","# Split into train and test (Random split)\n","from sklearn.model_selection import train_test_split\n","# Test-size: 30%, Train-size: 70%\n","train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1)\n","print(\"Training and test data dimensions: \\n train_x, train_y, test_x, test_y: \\n\",\n","      train_x.shape, test_x.shape, train_y.shape, test_y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AzNKeFJlP_3J","executionInfo":{"status":"ok","timestamp":1652288779228,"user_tz":420,"elapsed":507,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"}},"outputId":"9050447c-422d-4900-830b-d005921e5c59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Variables to be tested: ['Ta', 'Wind', 'SR', 'HR', 'DY', 'Flow', 'pp', 'SF', 'Tw_59']\n","Data dimension (X), (y): (5850, 8) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 8) (1755, 8) (4095,) (1755,)\n"]}]},{"cell_type":"code","source":["### MLP-Model definition with best parameters (This setting is not automatically updated)\n","def best_nn_model(n_inp):\n","  act_win = 'LeakyReLU'\n","  weight_win = 'uniform'\n","\n","  n_input = n_inp #len(train_x[0]) # Number of input variables\n","  b_model = Sequential()\n","  ### Input layer 0\n","  b_model.add(Dense(250, activation = act_win, input_dim=n_input, kernel_initializer = weight_win))\n","  #b_model.add(BatchNormalization())\n","\n","  ### Hidden layer 1\n","  #b_model.add(Dense(250, activation = act_win))\n","  #b_model.add(Dense(250, activation = act_win))\n","  #b_model.add(Dense(250, activation = act_win))\n","  #model.add(Dropout(drop_rate, seed=123))\n","\n","  ### Hidden layer 2\n","  b_model.add(Dense(250, activation = act_win))\n","  b_model.add(Dense(250, activation = act_win))\n","  #b_model.add(Dense(250, activation = act_win))\n","\n","  ### Output layer\n","  b_model.add(Dense(1))\n","\n","  #b_model.compile(loss='mse', optimizer=Adagrad(learning_rate = 0.005))\n","  b_model.compile(loss='mse', optimizer=Adamax(learning_rate = 0.005))\n","  return b_model\n"],"metadata":{"id":"l-j8YOxhTAqF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Get combination of children\n","import itertools, copy, heapq\n","\n","data = raw_d.drop(['scenario'],axis=1)\n","\n","n_vars = 8\n","root = ['Ta']\n","listvar = ['Flow', 'DY', 'SF','SR','pp','HR','Wind']\n","listvar2 = copy.deepcopy(listvar)\n","c72 = list(itertools.combinations(listvar2,n_vars-1))\n","print (c72)\n","print (len(c72))\n","set_n = []\n","sets = []\n","for sid in c72:\n","  #nnsid = root + list(sid)+['Tw']\n","  nnsid = root + list(sid)+['Tw_59']\n","  print (\"set_of_vars:\", nnsid)\n","  set_n.append(nnsid)\n","  st_d = data[nnsid]\n","  sets.append(st_d)\n","\n","print (len(sets))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tfL9AlYYliDz","executionInfo":{"status":"ok","timestamp":1652293384282,"user_tz":420,"elapsed":267,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"}},"outputId":"168e5cf7-cae6-4d80-81d6-6a74bbd34963"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('Flow', 'DY', 'SF', 'SR', 'pp', 'HR', 'Wind')]\n","1\n","set_of_vars: ['Ta', 'Flow', 'DY', 'SF', 'SR', 'pp', 'HR', 'Wind', 'Tw_59']\n","1\n"]}]},{"cell_type":"code","source":["st22 = ['Ta', 'DY', 'SR', 'Wind', 'Tw_59']\n","st23 = ['Ta', 'DY', 'pp', 'HR', 'Tw_59']\n","st24 = ['Ta', 'DY', 'pp', 'Wind', 'Tw_59']\n","st25 = ['Ta', 'DY', 'HR', 'Wind', 'Tw_59']\n","st26 =  ['Ta', 'SF', 'SR', 'pp', 'Tw_59']\n","st27 =  ['Ta', 'SF', 'SR', 'HR', 'Tw_59']\n","st28 =  ['Ta', 'SF', 'SR', 'Wind', 'Tw_59']\n","st29 =  ['Ta', 'SF', 'pp', 'HR', 'Tw_59']\n","st30 =  ['Ta', 'SF', 'pp', 'Wind', 'Tw_59']\n","st31 =  ['Ta', 'SF', 'HR', 'Wind', 'Tw_59']\n","st32 =  ['Ta', 'SR', 'pp', 'HR', 'Tw_59']\n","st33 =  ['Ta', 'SR', 'pp', 'Wind', 'Tw_59']\n","st34 =  ['Ta', 'SR', 'HR', 'Wind', 'Tw_59']\n","st35 =  ['Ta', 'pp', 'HR', 'Wind', 'Tw_59']\n","\n","set_n = [st22, st23, st24, st25, st26, st27, st28, st29, st30\n","         ,st31, st32, st33, st34, st35]\n","\n","st_d22 = data[st22]\n","st_d23 = data[st23]\n","st_d24 = data[st24]\n","st_d25 = data[st25]\n","st_d26 = data[st26]\n","st_d27 = data[st27]\n","st_d28 = data[st28]\n","st_d29 = data[st29]\n","st_d30 = data[st30]\n","st_d31 = data[st31]\n","st_d32 = data[st32]\n","st_d33 = data[st33]\n","st_d34 = data[st34]\n","st_d35 = data[st35]\n","\n","sets = [st_d22, st_d23, st_d24, st_d25, st_d26, st_d27, st_d28, st_d29, st_d30\n","         ,st_d31, st_d32, st_d33, st_d34, st_d35]\n"],"metadata":{"id":"sp73ri5_mBH_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from termcolor import colored\n","from sklearn.model_selection import train_test_split\n","\n","data = raw_d.drop(['scenario'],axis=1)\n","\n","#sets = [set1,set2,set3,set4,set5,set6,set7,set8,set9,set10,set11]\n","#sets = [set4]\n","labels=[str(r) for r in sets]\n","sets_names = [f'M{i}' for i in range(1, 12)]\n","#print(\"Models to be tested: \", sets_names)\n","\n","names = set_n\n","\n","scores_all_test_sets = []\n","scores_all_train_sets = []\n","ccc = 0\n","for sett in sets:\n","  #print (\"Model M\"+str(ccc+1)+\"......\", 'red')\n","  print (colored(\"Model M\"+str(ccc+1)+\"......\"+str(names[ccc][:-1]), 'red'))\n","  sett = np.asarray(sett, dtype=np.float32) # Convert pandas df to np array\n","\n","  ### split into Predictor and Response variables\n","  Xn = sett[:, :-1]\n","  yn = sett[:, -1]\n","  print (\"Data dimension (Xn), (yn):\", Xn.shape, yn.shape)\n","\n","  # Split into train and test (Random split)\n","\n","  # Test-size: 30%, Train-size: 70%\n","  train_xn, test_xn, train_yn, test_yn = train_test_split(Xn, yn, test_size=0.3, random_state=1)\n","  print(\"Training and test data dimensions: \\n train_x, train_y, test_x, test_y: \\n\",\n","        train_xn.shape, test_xn.shape, train_yn.shape, test_yn.shape)\n","  #print (\"train_x:\",train_xn)\n","  #print (\"train_y:\",train_yn)\n","\n","  n_epochs = 1500   #  <-----  you might SET\n","  batch_size = 100\n","\n","  all_loss, all_val_loss = [],[]\n","  all_preds, all_scores_train, all_scores_test = [], [], []\n","\n","  n_repeats = 5 #20  #     <----------------------- SET\n","  for i in range(n_repeats):\n","    print (\"Sim\", i+1)\n","    b_model = best_nn_model(len(train_xn[0]))   #  -----> Function model\n","    early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1, mode='auto') #We will wait 'patience=10' epochs before training is stopped\n","    hist_model = b_model.fit(train_xn, train_yn,validation_data=(test_xn, test_yn),\n","                          epochs=n_epochs, batch_size=batch_size, verbose=0, callbacks=[early_stop])\n","    #print(hist_model.history['loss']) # Get list of 'loss' at each epoch\n","    all_loss.append(hist_model.history['loss']) # For other error reports see: https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n","    all_val_loss.append(hist_model.history['val_loss']) # This loss is from the 'test' data\n","\n","    print (\"Loss (MSE):   \", hist_model.history['loss'][-1]) # This is MSE for train\n","    print (\"Val_loss (MSE):\", hist_model.history['val_loss'][-1]) # This is MSE for test\n","\n","\n","    ### Prediction\n","    ### Prediction on train\n","    pred_yn_train = b_model.predict(train_xn, verbose=0)\n","    pred_yn_train_flat = [item for sublist in pred_yn_train.tolist() for item in sublist]\n","    #print (\"Prediction on train data (Sim\",i+1, \") \\n\", pred_yn_train_flat)\n","    all_preds.append(pred_yn_train_flat)\n","    #print (dasda)\n","    #pred_y_test_flat.to_csv('predict.csv')  # save updated dataset\n","\n","    # Prediction Error. Root mean squared error or RMSE\n","    error_train = sqrt(mean_squared_error(train_yn, pred_yn_train))#error = measure_rmse(test, predictions)\n","    #print(\"RMSE train (sim\",i+1,\"):\",'  %.3f' % error_train) #E:\n","\n","    all_scores_train.append(error_train) #all_scores.append(scores)\n","    #print (all_scores_train)\n","\n","    ### Prediction on test\n","    pred_yn_test = b_model.predict(test_xn, verbose=0)\n","    pred_yn_test_flat = [item for sublist in pred_yn_test.tolist() for item in sublist]\n","    #print (\"Prediction on test data (Sim\",i+1, \") \\n\", pred_yn_test_flat)\n","    all_preds.append(pred_yn_test_flat)\n","    #print (dasdsa)\n","    #pred_y_test_flat.to_csv('predict.csv')  # save updated dataset\n","\n","    # Prediction Error. Root mean squared error or RMSE\n","    error_test = sqrt(mean_squared_error(test_yn, pred_yn_test))#error = measure_rmse(test, predictions)\n","    #print(\"RMSE test(sim\",i+1,\"):\",'   %.3f' % error_test) #E:\n","\n","    all_scores_test.append(error_test) #all_scores.append(scores)\n","    #print (all_scores_test)\n","    #print (ewewe)\n","\n","  #print (\"Tested variables (Header):\", var_tested) # print column names\n","  # summarize and plot scores\n","  print (\"Note: loss is MSE\")\n","  scores_m, score_std = mean(all_scores_test), std(all_scores_test)\n","  print('%s: %.3f RMSE_avge (+/- %.3f)' % ('Average of '+str(n_repeats)+\" repetitions\", scores_m, score_std))\n","\n","  scores_all_train_sets.append(all_scores_train) #Store scores of all models (M1, M2,...) as list od lists\n","  scores_all_test_sets.append(all_scores_test) #Store scores of all models (M1, M2,...) as list od lists\n","  print ()\n","  ccc += 1\n","\n","print (scores_all_train_sets)\n","print (scores_all_test_sets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UOHjuwfHrhpY","executionInfo":{"status":"ok","timestamp":1652293979219,"user_tz":420,"elapsed":583075,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"}},"outputId":"695cb169-6f92-4e02-8ba4-8787baf3e4af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mModel M1......['Ta', 'Flow', 'DY', 'SF', 'SR', 'pp', 'HR', 'Wind']\u001b[0m\n","Data dimension (Xn), (yn): (5850, 8) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 8) (1755, 8) (4095,) (1755,)\n","Sim 1\n","Epoch 141: early stopping\n","Loss (MSE):    0.8038082718849182\n","Val_loss (MSE): 0.8468785285949707\n","Sim 2\n","Epoch 423: early stopping\n","Loss (MSE):    0.4559570848941803\n","Val_loss (MSE): 0.706658124923706\n","Sim 3\n","Epoch 465: early stopping\n","Loss (MSE):    0.36168575286865234\n","Val_loss (MSE): 0.5864307284355164\n","Sim 4\n","Epoch 345: early stopping\n","Loss (MSE):    0.5108231902122498\n","Val_loss (MSE): 0.7359461188316345\n","Sim 5\n","Epoch 364: early stopping\n","Loss (MSE):    0.494468092918396\n","Val_loss (MSE): 0.7286659479141235\n","Note: loss is MSE\n","Average of 5 repetitions: 0.848 RMSE_avge (+/- 0.049)\n","\n","[[0.875639341292745, 0.6816903344859014, 0.5951431143571235, 0.7390927088815922, 0.7195971927806473]]\n","[[0.920260065524749, 0.8406296003137803, 0.765787691230514, 0.8578730202259741, 0.853619287685956]]\n"]}]},{"cell_type":"code","source":["print (scores_all_train_sets)\n","print (scores_all_test_sets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"02zb8Yveamn1","executionInfo":{"status":"ok","timestamp":1652260803993,"user_tz":420,"elapsed":135,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"}},"outputId":"2644704d-7954-4b40-93b4-c21ace4aee18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.9342634600148738, 0.8827778632196837, 0.874309130812743, 0.8494192678046883, 0.8328752848306805], [0.9993820367199177, 0.8248822402717277, 0.8074815629324675, 0.9172226166379335, 0.9470017842434084], [1.0360942117905454, 0.9100824743666827, 0.849840856127433, 0.9098772595494521, 0.9406048541272078], [0.9723866100449368, 0.942659675368085, 0.9100736981720676, 0.9034234725195656, 0.935317519072816], [1.1178179475919772, 1.0937217708759448, 1.1191380152913235, 1.062407545667302, 1.2117619814806713], [1.128091485114772, 1.0968471817010303, 1.1220565542733816, 1.154323622753495, 1.101462122895805], [1.114304617618379, 1.055790303434086, 1.0327540901969265, 1.0806448417152517, 1.1025173837238436], [1.1452056668404167, 1.0781224568654901, 1.2957791673842949, 1.1377960553404458, 1.1342798135297854], [1.0816531295301537, 1.250631125386044, 1.0991957650563924, 1.092449177622683, 1.0733350413370069], [1.1058236351153268, 1.1668600989249982, 1.1331944841057418, 1.1841234085876087, 1.220626315942886], [1.0259291043317116, 1.1002815861518214, 1.1034487543690812, 1.1039714040784523, 1.0541650679102499], [1.0968056637838248, 1.1578253247249617, 1.0991764063159006, 1.0620305202179119, 1.1107205572113326], [0.9898773392395442, 0.961115161630576, 1.1067736038472524, 1.1324887043134886, 1.0649926734278297], [1.1535128078303571, 1.1487092715111284, 1.114066666733064, 1.1156895872595225, 1.0974216414091729], [1.1556012163453584, 1.1149137072677981, 1.107122633129638, 1.15803801982443, 1.0994374223762968], [1.1113333274572044, 1.1128804874192757, 1.12156268689783, 1.1161464829412586, 1.1840104480617495], [1.2719951488534322, 1.2630585917174013, 1.345071320397726, 1.2840593753051617, 1.225392603728205], [1.3434374246111447, 1.2715762994354487, 1.2925030195101326, 1.510720172073146, 1.2750766244938727], [1.276296390359113, 1.2725400980569217, 1.2744744414426536, 1.2999471561988998, 1.2702750037408372], [1.287233430617616, 1.3733715037177, 1.3343084564412167, 1.4299321199858872, 1.3745230367679007], [1.0563565646255542, 1.075498782371353, 1.16302423449627, 1.0982233394909753, 1.114566208657357], [1.1075293002762767, 1.1107195376130519, 1.1058864815211784, 1.1247020962788972, 1.1153377344767064], [1.1000434195012043, 1.212713600299728, 1.0292417449953695, 1.094469651116451, 1.084457596593151], [1.1372199772334979, 1.10522274879213, 1.1663021074370366, 1.1109389979785633, 1.1328108162703003], [1.1749024005693611, 1.1751834705853585, 1.1221625785641747, 1.159291519902294, 1.1252641367775391], [1.2588674256028056, 1.0705533278793529, 1.1496585401209292, 1.209880441323337, 1.1295345366400202], [1.269522611148492, 1.2394371534220765, 1.3440602410161562, 1.437877190902037, 1.2639459846278578], [1.2362519974494743, 1.3296121685774938, 1.2489478928758226, 1.1961929967805676, 1.2842644377036676], [1.4429566023996325, 1.2830308307888871, 1.2619535623310072, 1.2626199721940683, 1.3015191445336662], [1.2693578983208562, 1.2739382736277818, 1.2787992718135799, 1.3024355704394264, 1.2192377923820106], [1.2638164832552938, 1.335035142043127, 1.273540703271674, 1.2266352443626864, 1.2568929402926663], [1.223382118891163, 1.2121638333438605, 1.1996434854361329, 1.2123898564285942, 1.3123942741317969], [1.2600698177733851, 1.4090306658605802, 1.2379501333284924, 1.2889881921794093, 1.3263365877812536], [1.267714341754618, 1.2726409386638176, 1.3092929623197838, 1.2208939792208084, 1.280078072849118], [1.442812515334802, 1.4933851937814353, 1.5105359085193948, 1.3892368717832972, 1.4450289061403923]]\n","[[0.9796040890364939, 0.9256582057645418, 0.9242737483656437, 0.9027473570923108, 0.8949988862781948], [1.052746717844213, 0.903502475825909, 0.8975285783749654, 0.9895075852699476, 0.9832537183751907], [1.0727638518506593, 0.9484846628142556, 0.9171828131742807, 0.9441915754807396, 0.961342547888603], [0.9950478242278661, 0.996216740214565, 0.9614057873012273, 0.9448777136826156, 0.9767072646607399], [1.17513051829946, 1.1654638777330673, 1.1918228171596823, 1.1391536158158524, 1.2429940825925305], [1.1741878320172983, 1.1661287952499955, 1.1748860141575959, 1.2076535395432542, 1.1668307268501996], [1.1838614283869182, 1.171377814045588, 1.1426327423344191, 1.1647614294548805, 1.1685403299190833], [1.196704179012907, 1.1441796709966905, 1.3705872907204324, 1.188457504533413, 1.2007096556526182], [1.152369612064132, 1.3279301668759016, 1.1898277456969293, 1.1575016773121094, 1.1435938827612557], [1.1741729077637395, 1.2143424615903462, 1.1798130499112067, 1.2179834449375857, 1.2579696272805636], [1.0810544669574076, 1.1485371449118167, 1.1484335555443528, 1.1564483987863363, 1.1137739195647327], [1.1281120384084584, 1.1949126129439893, 1.1322869495807129, 1.1182463103450333, 1.135974706069323], [1.0570409443051967, 1.0488148267531598, 1.136555663269978, 1.1600913597994598, 1.1017684202963345], [1.1966433627127233, 1.208010131537664, 1.1630817864870038, 1.156822526793812, 1.1513351423028084], [1.1931306960148276, 1.1721386422185864, 1.1572435595072967, 1.2101982074948074, 1.1508223495861587], [1.1400895102813429, 1.1472055483743737, 1.1552786484356539, 1.1465963539663089, 1.2050927654347705], [1.3254375868801906, 1.326335374419457, 1.4152307058103053, 1.3632247169818237, 1.2999521998640786], [1.39704036715218, 1.334666891579763, 1.346515426963826, 1.5594936440385092, 1.3506798921840095], [1.31619025476103, 1.3187132409925193, 1.3133215376684078, 1.3444430316220541, 1.3108253694758407], [1.3449369112271676, 1.4167311971590313, 1.3780926689801651, 1.473787151771105, 1.4081362894945804], [1.0691068778462889, 1.0717816259346402, 1.1710105250575387, 1.0820221057050994, 1.1085406644879128], [1.1084707093728718, 1.089558499103676, 1.0949317542456165, 1.1197381945714002, 1.1020882616611947], [1.1152632353834668, 1.2061603947697304, 1.0662423591295271, 1.0965341282867946, 1.0879936237989845], [1.1136120221161843, 1.0969923190332207, 1.134825345435966, 1.094441059325696, 1.1213156982192558], [1.1521743391261514, 1.149642519743644, 1.117977369956341, 1.1862968573847537, 1.1242465569170075], [1.2324861004956766, 1.0914746185930468, 1.139858772330487, 1.1885061017442704, 1.1124401462184188], [1.2842091604121435, 1.2652321193918754, 1.3483046146834718, 1.4324237092159564, 1.2816104847004095], [1.266105419389036, 1.3360261943442115, 1.2793429067934998, 1.258717039902226, 1.2980233129750063], [1.4242942266924532, 1.2838137493258392, 1.2824170914559876, 1.2846364184528463, 1.3163431305284456], [1.2853995327706658, 1.2820412588339023, 1.2976169072663073, 1.3193492196951238, 1.2605859259250516], [1.2541902404715675, 1.3257242382696595, 1.2651624422980676, 1.2236500561496988, 1.243002138582509], [1.213580833534293, 1.2043305339434034, 1.188284464273112, 1.2278957046119263, 1.3101629613023171], [1.2416401742929442, 1.393787089311937, 1.2255066621833544, 1.276724989225357, 1.3190379113221602], [1.255333346819831, 1.256506057758051, 1.2919849905907828, 1.2289347456764574, 1.260691599639843], [1.4452378537258714, 1.491252426439663, 1.511011160127033, 1.405861864567381, 1.4453266040010737]]\n"]}]},{"cell_type":"code","source":["from termcolor import colored\n","from sklearn.model_selection import train_test_split\n","\n","data = raw_d.drop(['scenario'],axis=1)\n","\n","#sets = [set1,set2,set3,set4,set5,set6,set7,set8,set9,set10,set11]\n","#sets = [set4]\n","labels=[str(r) for r in sets]\n","sets_names = [f'M{i}' for i in range(1, 12)]\n","#print(\"Models to be tested: \", sets_names)\n","\n","names = set_n\n","\n","scores_all_test_sets = []\n","scores_all_train_sets = []\n","ccc = 0\n","for sett in sets:\n","  #print (\"Model M\"+str(ccc+1)+\"......\", 'red')\n","  print (colored(\"Model M\"+str(ccc+1)+\"......\"+str(names[ccc][:-1]), 'red'))\n","  sett = np.asarray(sett, dtype=np.float32) # Convert pandas df to np array\n","\n","  ### split into Predictor and Response variables\n","  Xn = sett[:, :-1]\n","  yn = sett[:, -1]\n","  print (\"Data dimension (Xn), (yn):\", Xn.shape, yn.shape)\n","\n","  # Split into train and test (Random split)\n","\n","  # Test-size: 30%, Train-size: 70%\n","  train_xn, test_xn, train_yn, test_yn = train_test_split(Xn, yn, test_size=0.3, random_state=1)\n","  print(\"Training and test data dimensions: \\n train_x, train_y, test_x, test_y: \\n\",\n","        train_xn.shape, test_xn.shape, train_yn.shape, test_yn.shape)\n","  #print (\"train_x:\",train_xn)\n","  #print (\"train_y:\",train_yn)\n","\n","  n_epochs = 1500   #  <-----  you might SET\n","  batch_size = 100\n","\n","  all_loss, all_val_loss = [],[]\n","  all_preds, all_scores_train, all_scores_test = [], [], []\n","\n","  n_repeats = 5 #20  #     <----------------------- SET\n","  for i in range(n_repeats):\n","    print (\"Sim\", i+1)\n","    b_model = best_nn_model(len(train_xn[0]))   #  -----> Function model\n","    early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1, mode='auto') #We will wait 'patience=10' epochs before training is stopped\n","    hist_model = b_model.fit(train_xn, train_yn,validation_data=(test_xn, test_yn),\n","                          epochs=n_epochs, batch_size=batch_size, verbose=0, callbacks=[early_stop])\n","    #print(hist_model.history['loss']) # Get list of 'loss' at each epoch\n","    all_loss.append(hist_model.history['loss']) # For other error reports see: https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n","    all_val_loss.append(hist_model.history['val_loss']) # This loss is from the 'test' data\n","\n","    print (\"Loss (MSE):   \", hist_model.history['loss'][-1]) # This is MSE for train\n","    print (\"Val_loss (MSE):\", hist_model.history['val_loss'][-1]) # This is MSE for test\n","\n","\n","    ### Prediction\n","    ### Prediction on train\n","    pred_yn_train = b_model.predict(train_xn, verbose=0)\n","    pred_yn_train_flat = [item for sublist in pred_yn_train.tolist() for item in sublist]\n","    #print (\"Prediction on train data (Sim\",i+1, \") \\n\", pred_yn_train_flat)\n","    all_preds.append(pred_yn_train_flat)\n","    #print (dasda)\n","    #pred_y_test_flat.to_csv('predict.csv')  # save updated dataset\n","\n","    # Prediction Error. Root mean squared error or RMSE\n","    error_train = sqrt(mean_squared_error(train_yn, pred_yn_train))#error = measure_rmse(test, predictions)\n","    #print(\"RMSE train (sim\",i+1,\"):\",'  %.3f' % error_train) #E:\n","\n","    all_scores_train.append(error_train) #all_scores.append(scores)\n","    #print (all_scores_train)\n","\n","    ### Prediction on test\n","    pred_yn_test = b_model.predict(test_xn, verbose=0)\n","    pred_yn_test_flat = [item for sublist in pred_yn_test.tolist() for item in sublist]\n","    #print (\"Prediction on test data (Sim\",i+1, \") \\n\", pred_yn_test_flat)\n","    all_preds.append(pred_yn_test_flat)\n","    #print (dasdsa)\n","    #pred_y_test_flat.to_csv('predict.csv')  # save updated dataset\n","\n","    # Prediction Error. Root mean squared error or RMSE\n","    error_test = sqrt(mean_squared_error(test_yn, pred_yn_test))#error = measure_rmse(test, predictions)\n","    #print(\"RMSE test(sim\",i+1,\"):\",'   %.3f' % error_test) #E:\n","\n","    all_scores_test.append(error_test) #all_scores.append(scores)\n","    #print (all_scores_test)\n","    #print (ewewe)\n","\n","  #print (\"Tested variables (Header):\", var_tested) # print column names\n","  # summarize and plot scores\n","  print (\"Note: loss is MSE\")\n","  scores_m, score_std = mean(all_scores_test), std(all_scores_test)\n","  print('%s: %.3f RMSE_avge (+/- %.3f)' % ('Average of '+str(n_repeats)+\" repetitions\", scores_m, score_std))\n","\n","  scores_all_train_sets.append(all_scores_train) #Store scores of all models (M1, M2,...) as list od lists\n","  scores_all_test_sets.append(all_scores_test) #Store scores of all models (M1, M2,...) as list od lists\n","  print ()\n","  ccc += 1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"68wQK_vpTASC","executionInfo":{"status":"ok","timestamp":1652292664102,"user_tz":420,"elapsed":3847213,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"}},"outputId":"22017a6f-e3a7-4f86-eaaf-6625ceb25a89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mModel M1......['Ta', 'Flow', 'DY', 'SF', 'SR', 'pp', 'HR']\u001b[0m\n","Data dimension (Xn), (yn): (5850, 7) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 7) (1755, 7) (4095,) (1755,)\n","Sim 1\n","Epoch 134: early stopping\n","Loss (MSE):    0.9067230224609375\n","Val_loss (MSE): 0.8429430723190308\n","Sim 2\n","Epoch 165: early stopping\n","Loss (MSE):    0.7872020602226257\n","Val_loss (MSE): 0.8092048168182373\n","Sim 3\n","Epoch 164: early stopping\n","Loss (MSE):    0.8225869536399841\n","Val_loss (MSE): 1.2283614873886108\n","Sim 4\n","Epoch 369: early stopping\n","Loss (MSE):    0.6082757711410522\n","Val_loss (MSE): 0.7379276156425476\n","Sim 5\n","Epoch 260: early stopping\n","Loss (MSE):    0.7366646528244019\n","Val_loss (MSE): 0.8136477470397949\n","Note: loss is MSE\n","Average of 5 repetitions: 0.937 RMSE_avge (+/- 0.088)\n","\n","\u001b[31mModel M2......['Ta', 'Flow', 'DY', 'SF', 'SR', 'pp', 'Wind']\u001b[0m\n","Data dimension (Xn), (yn): (5850, 7) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 7) (1755, 7) (4095,) (1755,)\n","Sim 1\n","Epoch 205: early stopping\n","Loss (MSE):    0.6995551586151123\n","Val_loss (MSE): 0.8154934048652649\n","Sim 2\n","Epoch 277: early stopping\n","Loss (MSE):    0.6036345958709717\n","Val_loss (MSE): 0.7218317985534668\n","Sim 3\n","Epoch 215: early stopping\n","Loss (MSE):    0.6892385482788086\n","Val_loss (MSE): 0.7610154747962952\n","Sim 4\n","Epoch 273: early stopping\n","Loss (MSE):    0.6457229852676392\n","Val_loss (MSE): 0.9214600920677185\n","Sim 5\n","Epoch 382: early stopping\n","Loss (MSE):    0.46534255146980286\n","Val_loss (MSE): 0.7067977786064148\n","Note: loss is MSE\n","Average of 5 repetitions: 0.885 RMSE_avge (+/- 0.043)\n","\n","\u001b[31mModel M3......['Ta', 'Flow', 'DY', 'SF', 'SR', 'HR', 'Wind']\u001b[0m\n","Data dimension (Xn), (yn): (5850, 7) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 7) (1755, 7) (4095,) (1755,)\n","Sim 1\n","Epoch 249: early stopping\n","Loss (MSE):    0.7093074917793274\n","Val_loss (MSE): 0.847566545009613\n","Sim 2\n","Epoch 457: early stopping\n","Loss (MSE):    0.43660038709640503\n","Val_loss (MSE): 0.6111960411071777\n","Sim 3\n","Epoch 409: early stopping\n","Loss (MSE):    0.508229672908783\n","Val_loss (MSE): 0.7402089834213257\n","Sim 4\n","Epoch 333: early stopping\n","Loss (MSE):    0.6163237690925598\n","Val_loss (MSE): 0.7078149318695068\n","Sim 5\n","Epoch 562: early stopping\n","Loss (MSE):    0.34415289759635925\n","Val_loss (MSE): 0.5780913233757019\n","Note: loss is MSE\n","Average of 5 repetitions: 0.833 RMSE_avge (+/- 0.057)\n","\n","\u001b[31mModel M4......['Ta', 'Flow', 'DY', 'SF', 'pp', 'HR', 'Wind']\u001b[0m\n","Data dimension (Xn), (yn): (5850, 7) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 7) (1755, 7) (4095,) (1755,)\n","Sim 1\n","Epoch 443: early stopping\n","Loss (MSE):    0.49874749779701233\n","Val_loss (MSE): 0.7281256914138794\n","Sim 2\n","Epoch 145: early stopping\n","Loss (MSE):    0.8456928730010986\n","Val_loss (MSE): 0.9014614224433899\n","Sim 3\n","Epoch 199: early stopping\n","Loss (MSE):    0.7625471353530884\n","Val_loss (MSE): 0.7938770055770874\n","Sim 4\n","Epoch 237: early stopping\n","Loss (MSE):    0.7080028057098389\n","Val_loss (MSE): 0.7673817873001099\n","Sim 5\n","Epoch 217: early stopping\n","Loss (MSE):    0.756415069103241\n","Val_loss (MSE): 0.8280664086341858\n","Note: loss is MSE\n","Average of 5 repetitions: 0.896 RMSE_avge (+/- 0.033)\n","\n","\u001b[31mModel M5......['Ta', 'Flow', 'DY', 'SR', 'pp', 'HR', 'Wind']\u001b[0m\n","Data dimension (Xn), (yn): (5850, 7) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 7) (1755, 7) (4095,) (1755,)\n","Sim 1\n","Epoch 183: early stopping\n","Loss (MSE):    1.2579326629638672\n","Val_loss (MSE): 1.7994675636291504\n","Sim 2\n","Epoch 268: early stopping\n","Loss (MSE):    1.3048498630523682\n","Val_loss (MSE): 1.5195876359939575\n","Sim 3\n","Epoch 510: early stopping\n","Loss (MSE):    0.8564893007278442\n","Val_loss (MSE): 1.2215156555175781\n","Sim 4\n","Epoch 248: early stopping\n","Loss (MSE):    1.1209354400634766\n","Val_loss (MSE): 1.3678923845291138\n","Sim 5\n","Epoch 189: early stopping\n","Loss (MSE):    1.1694626808166504\n","Val_loss (MSE): 1.37625253200531\n","Note: loss is MSE\n","Average of 5 repetitions: 1.204 RMSE_avge (+/- 0.080)\n","\n","\u001b[31mModel M6......['Ta', 'Flow', 'SF', 'SR', 'pp', 'HR', 'Wind']\u001b[0m\n","Data dimension (Xn), (yn): (5850, 7) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 7) (1755, 7) (4095,) (1755,)\n","Sim 1\n","Epoch 459: early stopping\n","Loss (MSE):    0.6985170245170593\n","Val_loss (MSE): 0.9889823794364929\n","Sim 2\n","Epoch 534: early stopping\n","Loss (MSE):    0.6246399283409119\n","Val_loss (MSE): 0.8160533308982849\n","Sim 3\n","Epoch 419: early stopping\n","Loss (MSE):    0.7323028445243835\n","Val_loss (MSE): 0.9609256982803345\n","Sim 4\n","Epoch 292: early stopping\n","Loss (MSE):    0.8787699341773987\n","Val_loss (MSE): 1.0343817472457886\n","Sim 5\n","Epoch 195: early stopping\n","Loss (MSE):    1.0566160678863525\n","Val_loss (MSE): 1.143080711364746\n","Note: loss is MSE\n","Average of 5 repetitions: 0.993 RMSE_avge (+/- 0.054)\n","\n","\u001b[31mModel M7......['Ta', 'DY', 'SF', 'SR', 'pp', 'HR', 'Wind']\u001b[0m\n","Data dimension (Xn), (yn): (5850, 7) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 7) (1755, 7) (4095,) (1755,)\n","Sim 1\n","Epoch 477: early stopping\n","Loss (MSE):    0.661329448223114\n","Val_loss (MSE): 0.8670275211334229\n","Sim 2\n","Epoch 179: early stopping\n","Loss (MSE):    1.170258641242981\n","Val_loss (MSE): 1.090130090713501\n","Sim 3\n","Epoch 383: early stopping\n","Loss (MSE):    0.9845215678215027\n","Val_loss (MSE): 0.9198372960090637\n","Sim 4\n","Epoch 718: early stopping\n","Loss (MSE):    0.35963958501815796\n","Val_loss (MSE): 0.6596338152885437\n","Sim 5\n","Epoch 172: early stopping\n","Loss (MSE):    1.1437036991119385\n","Val_loss (MSE): 1.2167960405349731\n","Note: loss is MSE\n","Average of 5 repetitions: 0.970 RMSE_avge (+/- 0.100)\n","\n"]}]},{"cell_type":"code","source":["scores_all_test_sets"],"metadata":{"id":"4QRkYGhu6Z0L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Save output data as CSV file\n","from google.colab import files\n","import pandas as pd\n","scores_df = pd.DataFrame(set_n)\n","scores_df.to_csv('set_headers.csv')\n","scores_df = pd.DataFrame(scores_all_train_sets)\n","scores_df.to_csv('scores_mlp_train.csv')\n","scores_df = pd.DataFrame(scores_all_test_sets)\n","scores_df.to_csv('scores_mlp_test.csv')\n"],"metadata":{"id":"IEEiexc8vvnW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Resources to print as pdf"],"metadata":{"id":"2tptrspWP7Tz"}},{"cell_type":"code","source":["!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n","!pip install pypandoc\n"],"metadata":{"id":"b-ijQGOzROEa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic"],"metadata":{"id":"6vviWZqcRPvT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!jupyter nbconvert --to pdf /content/drive/MyDrive/Colab_Notebooks/MLP_models_for_classes/010_mlp_sb31_submit2.ipynb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_HaDBBxRGw3","executionInfo":{"status":"ok","timestamp":1650312280001,"user_tz":420,"elapsed":8178,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"}},"outputId":"d8c08669-9325-4bb1-b82d-8214c8cfc8e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook /content/drive/MyDrive/Colab_Notebooks/MLP_models_for_classes/010_mlp_sb31_submit2.ipynb to pdf\n","[NbConvertApp] Support files will be in 010_mlp_sb31_submit2_files/\n","[NbConvertApp] Making directory ./010_mlp_sb31_submit2_files\n","[NbConvertApp] Writing 90028 bytes to ./notebook.tex\n","[NbConvertApp] Building PDF\n","[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n","[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n","[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n","[NbConvertApp] PDF successfully created\n","[NbConvertApp] Writing 173069 bytes to /content/drive/MyDrive/Colab_Notebooks/MLP_models_for_classes/010_mlp_sb31_submit2.pdf\n"]}]}]}