{"cells":[{"cell_type":"markdown","metadata":{"id":"_D1gJLR_keDq"},"source":["## **Convolutional Neural Network (CNN) model for Forecasting ML Models**\n","\n","In this page, the CNN model is trained and tested for stream temperatura prediction considering the air-temperature (Ta), wind (Wind), solar radiation (SR), relative humidity (HR), the day in the year (DY), streamflow (Flow), precipitation (pp), and the shade factor (FS) as predictor variables. The daily stream temperature (Tw) is set as the only response variable."]},{"cell_type":"markdown","metadata":{"id":"PjF14qEDQmP8"},"source":["Clean variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhJGKnYBkdNI"},"outputs":[],"source":["### Clean variables\n","from IPython import get_ipython\n","get_ipython().magic('reset -sf')"]},{"cell_type":"markdown","metadata":{"id":"g5MihNa1QqKR"},"source":["### **Reading of data, process and separation in train and test**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1652325970978,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"},"user_tz":420},"id":"r4_EUQFkkpkS","outputId":"21ed9a04-aff8-41fd-e5f5-a5a234dde402"},"outputs":[{"name":"stdout","output_type":"stream","text":["Variables to be tested: ['Ta', 'Wind', 'SR', 'HR', 'DY', 'Flow', 'pp', 'SF', 'Tw']\n","Data dimension (X), (y): (6438, 8) (6438,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4506, 8) (1932, 8) (4506,) (1932,)\n"]}],"source":["### Read Data in CSV format for temporal analysis\n","from pandas import read_csv\n","import numpy as np\n","\n","# Dir:  D:\\research\\ML_model\\new_data\n","#raw_d = read_csv('006_sb31_8v_norm_2012_2018.csv', header=0, index_col=0)  # For SB 31\n","#raw_d = read_csv('006_sb59_8v_norm_2006_2012.csv', header=0, index_col=0) # For SB 59\n","raw_d = read_csv('002_sb31_norm_2012_2017_for_train_test.csv', header=0, index_col=0)  # For SB 31\n","\n","### Removing/Dropping no-needed variables\n","data = raw_d.drop(['scenario'],axis=1)\n","var_tested = list(data.columns.values)\n","print (\"Variables to be tested:\", list(data.columns.values)) # Print headers\n","\n","### convert df pandas to np-array\n","data = np.asarray(data, dtype=np.float32) # Convert pandas df to np array\n","\n","### split into Predictor and Response variables\n","X = data[:, :-1]\n","y = data[:, -1]\n","print (\"Data dimension (X), (y):\", X.shape, y.shape)\n","\n","# Split into train and test (Random split)\n","from sklearn.model_selection import train_test_split\n","# Test-size: 30%, Train-size: 70%\n","train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1)\n","print(\"Training and test data dimensions: \\n train_x, train_y, test_x, test_y: \\n\",\n","      train_x.shape, test_x.shape, train_y.shape, test_y.shape)"]},{"cell_type":"markdown","metadata":{"id":"6PbL3wKiYDA4"},"source":["### Main libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bNi9Snt6ktNs"},"outputs":[],"source":["import random\n","from numpy import mean, std\n","from math import sqrt\n","from keras.layers import Dense, BatchNormalization, Dropout\n","from keras.models import Sequential\n","from keras.callbacks import EarlyStopping\n","from sklearn.metrics import mean_squared_error\n","from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n"]},{"cell_type":"markdown","metadata":{"id":"kQI2_wVuRJki"},"source":["### **Tuning of Hyperparameters**\n","Eleven parameters were tuned:  Activation function,kernel_initializer (initial_weigths), optimizer, learning_rate, n_epochs, batch_size, number of neurons,first set of hidden layers (layers1), second set of hidden layers (layers2), nomalization, and dropout.\n","\n","Several iterations were performed to find the best set of hyperparameters.\n","\n","Running this part can take several hours (more than 30 hours in some occasions)\n","\n","Results are displayed in a table for a better comparison of the performance of hyperparameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zj8vbiDwm0kj"},"outputs":[],"source":["act_all, opt_all, l_rate_all, n_epochs_all, batch_size_all = [],[],[],[],[]\n","n_neur_all, layers1_all, layers2_all =[],[],[]\n","normaliz_d_all, drop_d_all, drop_rate_all = [],[],[]\n","perfm1, perfm2 = [],[]\n","\n","for i in range(10):\n","  print (\"Set # \"+str(i+1)+\" in process .................\")\n","  ### Set of parameters -------------------------------------------\n","  ### Parameters to be Tuned\n","  ### Activation,init_w,optimizer,learning_rate,batch_size,n_epochs,\n","  ### n_neur,layers1, layers2, nomaliz, dropout\n","\n","  ### Hyperparameters\n","  # 1. Activation selection\n","  activat_n = ['relu','sigmoid','softplus','softsign','tanh','selu','elu','exponential','LeakyReLU']\n","  #activat_n = ['relu']\n","  activat_id = random.randint(0, len(activat_n)-1)\n","  activat_s = activat_n[activat_id]\n","\n","  # 2. kernel_initializer (itial_weigths)\n","  init_w = ['normal','uniform','zeros']\n","  #init_w = ['uniform']\n","  init_w_id = random.randint(0, len(init_w)-1)\n","  init_w_s = init_w[init_w_id]\n","\n","  # 3 & 4. Optimizer and learning-rate selection\n","  l_rate = 0.005 # random.randint(1,200)/1000\n","  opt1 = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']\n","  #opt1 = ['Adagrad']\n","  opt_id = random.randint(0, len(opt1)-1)\n","  opt2= {'Adam':Adam(learning_rate=l_rate), 'SGD':SGD(learning_rate=l_rate),\n","                  'RMSprop':RMSprop(learning_rate=l_rate), 'Adadelta':Adadelta(learning_rate=l_rate),\n","                  'Adagrad':Adagrad(learning_rate=l_rate), 'Adamax':Adamax(learning_rate=l_rate),\n","                  'Nadam':Nadam(learning_rate=l_rate), 'Ftrl':Ftrl(learning_rate=l_rate)}\n","\n","  #print (\"opt_id:\",opt_id, \";   Optmzr:\", opt2[opt1[opt_id]].__dict__[\"_name\"])\n","\n","  # 5 & 6. n_epochs, batch_size,\n","  n_epochs = 1500 # random.randint(100, 500)\n","  batch_size = 100 # random.randint(100, 300)\n","\n","  # 7. number of neurons\n","  n_neur = 250 # random.randint(100, 300)\n","\n","  # 8 & 9 number of layers\n","  layers1 = random.randint(0, 3) # 1 #\n","  layers2 = random.randint(0, 3) # 1 #\n","\n","  # 10. nomalization\n","  normaliz_d = random.randint(0, 1) # 1 #\n","\n","  # 11. dropout\n","  drop_d = random.randint(0, 1) # 1 #\n","  drop_rate = random.randint(50, 80)/100 # 0.5 - 0.8 good values\n","  ### End set of parameters -----------------------------\n","\n","  ### Model definition\n","  #params = [activationL, init_weight, optimizerL, n_epochs, batch_size, learning_rate]\n","  n_input = len(train_x[0]) # Number of input variables\n","  model = Sequential()\n","\n","  ### Hidden layer 0\n","  #model.add(Dense(n_neur, activation = activat_s, input_dim=n_input, kernel_initializer =init_w_s))# 'uniform'))\n","\n","  ### CNN layer 0\n","  #b_model.add(Dense(250, activation = 'relu', input_dim=n_input, kernel_initializer = 'uniform'))\n","  filters= 128 # 256 # Suggested between 30 and 128\n","  kernel_size_cnn1 = 3\n","  kernel_size_cnn2 = 1\n","\n","\n","  if n_var < 3:\n","    kernel_size_cnn1 = 1\n","\n","  model.add(Conv1D(filters, 3, activation='relu', input_shape=(n_input, 1))) #n_filters=256, n_kernel=3\n","\n","  if conv2_d > 0.5:\n","    model.add(Conv1D(filters, 1, activation='relu'))\n","\n","  model.add(MaxPooling1D(padding='same'))\n","  model.add(Flatten())\n","\n","\n","  if normaliz_d > 0.5:\n","    model.add(BatchNormalization())\n","\n","  ### Hidden layer 1\n","  for i in range(layers1):\n","    model.add(Dense(n_neur, activation = activat_s))\n","    #model.add(Dense(n_neur, activation = activat_s))\n","    #model.add(Dense(n_neur, activation = activat_s))\n","\n","  if drop_d > 0.5:\n","    model.add(Dropout(drop_rate, seed=123))\n","\n","  ### Hidden layer 2\n","  for i in range(layers2):\n","    model.add(Dense(n_neur, activation = activat_s))\n","    #model.add(Dense(n_neur, activation = activat_s))\n","\n","  ### Output layer\n","  model.add(Dense(1)) #model.add(Dense(1, activation='sigmoid'))\n","  #model.compile(loss='mse', optimizer=params[2]) # Compiling the model\n","  #model.compile(loss='mse', optimizer=Adam(learning_rate = l_rate)) # 0.8279\n","  model.compile(loss='mse', optimizer = opt2[opt1[opt_id]]) # 0.8279\n","\n","  ### Fit the ML-Perceptron model\n","\n","  # More about 'EarlyStopping': https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n","  # \"batch_size\" must be > 1 and < n_samples. Ref: https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n","  early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1, mode='auto') #We will wait 'patience=10' epochs before training is stopped\n","  hist_model = model.fit(train_x, train_y,validation_data=(test_x, test_y),\n","                        epochs=n_epochs, batch_size=batch_size, verbose=0, callbacks=[early_stop])\n","  #print(hist_model.history['loss']) # Get list of 'loss' at each epoch\n","  #all_loss.append(hist_model.history['loss']) # For other error reports see: https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n","  #all_val_loss.append(hist_model.history['val_loss']) # This loss is from the 'test' data\n","\n","  print (activat_id, activat_n[activat_id])\n","  print (opt_id, opt2[opt1[opt_id]].__dict__[\"_name\"])\n","  print (\"learning_rate:\", l_rate)\n","  print (\"n_epochs, batch_size:\", n_epochs, batch_size)\n","  print (\"neurons, L1, L2: \", n_neur, layers1, layers2)\n","  #print (\"nomalztn:\", normaliz_d)\n","  print (\"nomalztn:\", (\"Yes\" if normaliz_d else \"No\"))\n","  #print (\"Dropout:\", drop_d, drop_rate)\n","  print (\"Dropout:\", (\"Yes\" if drop_d else \"No\"), \" \", (drop_rate if drop_d else \"\"))\n","  print (\"Performance\", round(hist_model.history['loss'][-1],4),\" \", round(hist_model.history['val_loss'][-1],4))\n","  print ()\n","\n","  act_all.append(activat_n[activat_id])\n","  opt_all.append(opt2[opt1[opt_id]].__dict__[\"_name\"])\n","  l_rate_all.append(l_rate)\n","  n_epochs_all.append(n_epochs)\n","  batch_size_all.append(batch_size)\n","  n_neur_all.append(n_neur)\n","  layers1_all.append(layers1)\n","  layers2_all.append(layers2)\n","  normaliz_d_all.append(\"Yes\" if normaliz_d else \"No\")\n","  drop_d_all.append(\"Yes\" if drop_d else \"No\")\n","  drop_rate_all.append(drop_rate)\n","  perfm1.append(round(hist_model.history['loss'][-1],3))\n","  perfm2.append(round(hist_model.history['val_loss'][-1],3))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGSGyiJd5DwW"},"outputs":[],"source":["### Print results of tuning\n","headers = ['Train_err','Test_err','Activat','optimz','learning_r','Epochs','Batch',\n","           'Neurons','Layers1','Layers2','Normalzn','Dropout','Drop_r']\n","scores = [perfm1,perfm2,act_all, opt_all, l_rate_all,\n","      n_epochs_all, batch_size_all, n_neur_all, layers1_all,\n","      layers2_all, normaliz_d_all, drop_d_all, drop_rate_all\n","      ]\n","scores_t = list(map(list, zip(*scores)))\n","#print (scores_t)\n","\n","from tabulate import tabulate\n","print(tabulate(scores_t, headers=headers, tablefmt='orgtbl'))"]},{"cell_type":"markdown","metadata":{"id":"KOua5vaLSq4s"},"source":["## **Modeling employing tuned parameters**\n","Using the model with tuned parameters, stream temperature prediction were performed for eleven sets of predictor variables."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWPEKbZ4p3Cq"},"outputs":[],"source":["import random\n","from numpy import mean, std\n","from math import sqrt\n","from keras.layers import Dense, BatchNormalization, Dropout\n","from keras.models import Sequential\n","from keras.callbacks import EarlyStopping\n","from sklearn.metrics import mean_squared_error\n","from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":145,"status":"ok","timestamp":1652494410190,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"},"user_tz":420},"id":"5eiS56KCpo5o","outputId":"9e11fc47-c2d6-46c7-898c-80821e35d9f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Variables to be tested: ['Ta', 'Wind', 'SR', 'HR', 'DY', 'Flow', 'pp', 'SF', 'Tw_59']\n","Data dimension (X), (y): (5850, 8) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 8) (1755, 8) (4095,) (1755,)\n"]}],"source":["### Read Data in CSV format for temporal analysis\n","from pandas import read_csv\n","import numpy as np\n","\n","# Dir:  D:\\research\\ML_model\\new_data\n","#raw_d = read_csv('006_sb31_8v_norm_2012_2018.csv', header=0, index_col=0)  # For SB 31\n","#raw_d = read_csv('006_sb59_8v_norm_2006_2012.csv', header=0, index_col=0) # For SB 59\n","#raw_d = read_csv('002_sb31_norm_2012_2017_for_train_test.csv', header=0, index_col=0)  # For SB 31\n","raw_d = read_csv('004_sb59_norm_2006_2011_for_train_test.csv', header=0, index_col=0)  # For SB 59\n","\n","### Removing/Dropping no-needed variables\n","data = raw_d.drop(['scenario'],axis=1)\n","var_tested = list(data.columns.values)\n","print (\"Variables to be tested:\", list(data.columns.values)) # Print headers\n","\n","### convert df pandas to np-array\n","data = np.asarray(data, dtype=np.float32) # Convert pandas df to np array\n","\n","### split into Predictor and Response variables\n","X = data[:, :-1]\n","y = data[:, -1]\n","print (\"Data dimension (X), (y):\", X.shape, y.shape)\n","\n","# Split into train and test (Random split)\n","from sklearn.model_selection import train_test_split\n","# Test-size: 30%, Train-size: 70%\n","train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1)\n","print(\"Training and test data dimensions: \\n train_x, train_y, test_x, test_y: \\n\",\n","      train_x.shape, test_x.shape, train_y.shape, test_y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"31QKEM3vOo2o"},"outputs":[],"source":["from keras.layers.convolutional import Conv1D, MaxPooling1D\n","from keras.layers import Flatten\n","\n","### MLP-Model definition with best parameters (This setting is not automatically updated)\n","def best_nn_model(n_inp):\n","  n_input = n_inp #len(train_x[0]) # Number of input variables\n","  b_model = Sequential()\n","\n","  ### CNN layer 0\n","  #b_model.add(Dense(250, activation = 'relu', input_dim=n_input, kernel_initializer = 'uniform'))\n","  filters= 128 # 256 # Suggested between 30 and 128\n","\n","  # The kernel_size_cnn1 must be <= than n_inputs. If n_iputs=2 => kernel_size_cnn1 =2 or 1\n","  #kernel_size_cnn1 = 3 # if n_predictors < 3 => kernel_size_cnn1 = n_predictors\n","  #kernel_size_cnn2 = 1\n","\n","  kernel_size_cnn1 = 3 if n_inp >= 3 else n_inp\n","  kernel_size_cnn2 = 3 if n_inp >= 5 else 1\n","  #! print (\"n_inp:\",n_inp, \"    kernel 1:\",kernel_size_cnn1, \"      kernel 2:\",kernel_size_cnn2)\n","\n","  b_model.add(Conv1D(filters, kernel_size_cnn1, activation='relu', input_shape=(n_input, 1))) #n_filters=256, n_kernel=3\n","  b_model.add(Conv1D(filters, kernel_size_cnn2, activation='relu'))\n","  b_model.add(MaxPooling1D(padding='same'))\n","  b_model.add(Flatten())\n","\n","  #b_model.add(BatchNormalization())\n","\n","  ### Hidden layer 1\n","  b_model.add(Dense(250, activation = 'relu'))\n","  b_model.add(Dense(250, activation = 'relu'))\n","  b_model.add(Dense(250, activation = 'relu'))\n","  #model.add(Dropout(drop_rate, seed=123))\n","\n","  ### Hidden layer 2\n","  b_model.add(Dense(250, activation = 'relu'))\n","  #b_model.add(Dense(250, activation = 'relu'))\n","  #b_model.add(Dense(250, activation = 'relu'))\n","\n","  ### Output layer\n","  b_model.add(Dense(1))\n","\n","  b_model.compile(loss='mse', optimizer=Adagrad(learning_rate = 0.005))\n","  return b_model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":177,"status":"ok","timestamp":1652502206018,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"},"user_tz":420},"id":"60IUPwoHpZQp","outputId":"8ca4748f-cdf3-4372-cae8-c8220ec8ebe6"},"outputs":[{"output_type":"stream","name":"stdout","text":["[()]\n","1\n","set_of_vars: ['Ta', 'Tw_59']\n","1\n"]}],"source":["### Get combination of children\n","import itertools, copy, heapq\n","\n","data = raw_d.drop(['scenario'],axis=1)\n","\n","n_vars = 1  # <------------- SET\n","root = ['Ta']\n","listvar = ['Flow', 'DY', 'SF','SR','pp','HR','Wind']\n","listvar2 = copy.deepcopy(listvar)\n","c72 = list(itertools.combinations(listvar2,n_vars-1))\n","print (c72)\n","print (len(c72))\n","set_n = []\n","sets = []\n","for sid in c72:\n","  #nnsid = root + list(sid)+['Tw']\n","  nnsid = root + list(sid)+['Tw_59']\n","  print (\"set_of_vars:\", nnsid)\n","  set_n.append(nnsid)\n","  st_d = data[nnsid]\n","  sets.append(st_d)\n","\n","print (len(sets))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":167,"status":"ok","timestamp":1652440337385,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"},"user_tz":420},"id":"PQN7q3d1rHN-","outputId":"66579cec-ea19-409e-8541-080e12252ba7"},"outputs":[{"data":{"text/plain":["[['Ta', 'Flow', 'SF', 'SR', 'Wind', 'Tw_59'],\n"," ['Ta', 'Flow', 'SF', 'pp', 'HR', 'Tw_59'],\n"," ['Ta', 'Flow', 'SF', 'pp', 'Wind', 'Tw_59'],\n"," ['Ta', 'Flow', 'SF', 'HR', 'Wind', 'Tw_59'],\n"," ['Ta', 'Flow', 'SR', 'pp', 'HR', 'Tw_59'],\n"," ['Ta', 'Flow', 'SR', 'pp', 'Wind', 'Tw_59'],\n"," ['Ta', 'Flow', 'SR', 'HR', 'Wind', 'Tw_59'],\n"," ['Ta', 'Flow', 'pp', 'HR', 'Wind', 'Tw_59'],\n"," ['Ta', 'DY', 'SF', 'SR', 'pp', 'Tw_59'],\n"," ['Ta', 'DY', 'SF', 'SR', 'HR', 'Tw_59'],\n"," ['Ta', 'DY', 'SF', 'SR', 'Wind', 'Tw_59'],\n"," ['Ta', 'DY', 'SF', 'pp', 'HR', 'Tw_59'],\n"," ['Ta', 'DY', 'SF', 'pp', 'Wind', 'Tw_59'],\n"," ['Ta', 'DY', 'SF', 'HR', 'Wind', 'Tw_59'],\n"," ['Ta', 'DY', 'SR', 'pp', 'HR', 'Tw_59'],\n"," ['Ta', 'DY', 'SR', 'pp', 'Wind', 'Tw_59'],\n"," ['Ta', 'DY', 'SR', 'HR', 'Wind', 'Tw_59'],\n"," ['Ta', 'DY', 'pp', 'HR', 'Wind', 'Tw_59'],\n"," ['Ta', 'SF', 'SR', 'pp', 'HR', 'Tw_59'],\n"," ['Ta', 'SF', 'SR', 'pp', 'Wind', 'Tw_59'],\n"," ['Ta', 'SF', 'SR', 'HR', 'Wind', 'Tw_59'],\n"," ['Ta', 'SF', 'pp', 'HR', 'Wind', 'Tw_59'],\n"," ['Ta', 'SR', 'pp', 'HR', 'Wind', 'Tw_59']]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["set_n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RKs9vKuxzug5","outputId":"13418cb8-7d03-4a49-b1d6-b1e8b210d22b","executionInfo":{"status":"ok","timestamp":1652502398701,"user_tz":420,"elapsed":181247,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mModel M1......['Ta']\u001b[0m\n","Data dimension (Xn), (yn): (5850, 1) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 1) (1755, 1) (4095,) (1755,)\n","Sim 1\n","Loss:    2.210084915161133\n","Val_loss: 2.1816301345825195\n","\n","Sim 2\n","Loss:    2.2057275772094727\n","Val_loss: 2.1699931621551514\n","\n","Sim 3\n","Loss:    2.2049720287323\n","Val_loss: 2.174150228500366\n","\n","Sim 4\n","Loss:    2.209399700164795\n","Val_loss: 2.1659343242645264\n","\n","Sim 5\n","Loss:    2.2306597232818604\n","Val_loss: 2.2189114093780518\n","\n","Note: loss is MSE\n","Average of 5 repetitions: 1.477 RMSE_avge (+/- 0.006)\n","\n","[[1.48825834161544, 1.4823845618657165, 1.4838341781789195, 1.4825316377194078, 1.499803450740757]]\n","[[1.4770341553816353, 1.4730897462726873, 1.4744999118622515, 1.4717114400191043, 1.4896012507430336]]\n"]}],"source":["from termcolor import colored\n","from sklearn.model_selection import train_test_split\n","data = raw_d.drop(['scenario'],axis=1)\n","\n","#sets = [set1,set2,set3,set4,set5,set6,set7]\n","labels=[str(r) for r in sets]\n","sets_names = [f'M{i}' for i in range(1, 12)]\n","#print(\"Models to be tested: \", sets_names)\n","\n","names = set_n\n","\n","scores_all_train_sets = []\n","scores_all_test_sets = []\n","ccc = 0\n","for sett in sets:\n","  #print (\"Model M\"+str(ccc+1)+\"......\")\n","  print (colored(\"Model M\"+str(ccc+1)+\"......\"+str(names[ccc][:-1]), 'red'))\n","  sett = np.asarray(sett, dtype=np.float32) # Convert pandas df to np array\n","\n","  ### split into Predictor and Response variables\n","  Xn = sett[:, :-1]\n","  yn = sett[:, -1]\n","  print (\"Data dimension (Xn), (yn):\", Xn.shape, yn.shape)\n","\n","  # Split into train and test (Random split)\n","\n","  # Test-size: 30%, Train-size: 70%\n","  train_xn, test_xn, train_yn, test_yn = train_test_split(Xn, yn, test_size=0.3, random_state=1)\n","  print(\"Training and test data dimensions: \\n train_x, train_y, test_x, test_y: \\n\",\n","        train_xn.shape, test_xn.shape, train_yn.shape, test_yn.shape)\n","\n","  n_epochs = 1500   #  <-----  you might SET\n","  batch_size = 100\n","\n","  all_loss, all_val_loss = [],[]\n","  all_preds, all_scores_train, all_scores_test = [], [], []\n","\n","  n_repeats = 5  #     <----------------------- SET\n","  for i in range(n_repeats):\n","    print (\"Sim\", i+1)\n","    b_model = best_nn_model(len(train_xn[0]))   #  -----> call model\n","    early_stop = EarlyStopping(monitor='loss', patience=20, verbose=0, mode='auto') #We will wait 'patience=10' epochs before training is stopped\n","    hist_model = b_model.fit(train_xn, train_yn,validation_data=(test_xn, test_yn),\n","                          epochs=n_epochs, batch_size=batch_size, verbose=0, callbacks=[early_stop])\n","    #print(hist_model.history['loss']) # Get list of 'loss' at each epoch\n","    all_loss.append(hist_model.history['loss']) # For other error reports see: https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n","    all_val_loss.append(hist_model.history['val_loss']) # This loss is from the 'test' data\n","\n","    print (\"Loss:   \", hist_model.history['loss'][-1])\n","    print (\"Val_loss:\", hist_model.history['val_loss'][-1])\n","\n","\n","    ### Prediction\n","    ### Prediction on train\n","    pred_yn_train = b_model.predict(train_xn, verbose=0)\n","    pred_yn_train_flat = [item for sublist in pred_yn_train.tolist() for item in sublist]\n","    #print (\"Prediction on train data (Sim\",j+1, \") \\n\", pred_y_train_flat)\n","    all_preds.append(pred_yn_train_flat)\n","    #pred_y_test_flat.to_csv('predict.csv')  # save updated dataset\n","\n","    # Prediction Error. Root mean squared error or RMSE\n","    error_train = sqrt(mean_squared_error(train_yn, pred_yn_train))#error = measure_rmse(test, predictions)\n","    #print(\"RMSE train (sim\",j+1,\"):\",'  %.3f' % error_train) #E:\n","\n","    all_scores_train.append(error_train) #all_scores.append(scores)\n","    #print (all_scores)\n","\n","    ### Prediction on test\n","    pred_yn_test = b_model.predict(test_xn, verbose=0)\n","    pred_yn_test_flat = [item for sublist in pred_yn_test.tolist() for item in sublist]\n","    #print (\"Prediction on test data (Sim\",j+1, \") \\n\", pred_y_test_flat)\n","    all_preds.append(pred_yn_test_flat)\n","    #pred_y_test_flat.to_csv('predict.csv')  # save updated dataset\n","\n","    # Prediction Error. Root mean squared error or RMSE\n","    error_test = sqrt(mean_squared_error(test_yn, pred_yn_test))#error = measure_rmse(test, predictions)\n","    #print(\"RMSE test(sim\",j+1,\"):\",'   %.3f' % error_test) #E:\n","\n","    all_scores_test.append(error_test) #all_scores.append(scores)\n","    #print (all_scores)\n","    print ()\n","\n","  #print (\"Tested variables (Header):\", var_tested) # print column names\n","  # summarize and plot scores\n","  print (\"Note: loss is MSE\")\n","  scores_m, score_std = mean(all_scores_test), std(all_scores_test)\n","  print('%s: %.3f RMSE_avge (+/- %.3f)' % ('Average of '+str(n_repeats)+\" repetitions\", scores_m, score_std))\n","\n","\n","  #all_sets.append(all_scores_test) #Store scores of all models (M1, M2,...) as list od lists\n","  scores_all_train_sets.append(all_scores_train) #Store scores of all models (M1, M2,...) as list od lists\n","  scores_all_test_sets.append(all_scores_test) #Store scores of all models (M1, M2,...) as list od lists\n","\n","  print ()\n","  ccc += 1\n","  if ccc%10 == 0:\n","    print (\"Preliminar print\")\n","    print (scores_all_train_sets)\n","    print (scores_all_test_sets)\n","\n","print (scores_all_train_sets)\n","print (scores_all_test_sets)"]},{"cell_type":"code","source":["from google.colab import files\n","import pandas as pd\n","all_sets_df = pd.DataFrame(scores_all_train_sets)\n","all_sets_df.to_csv('SB31_cnn_train.csv')\n","all_sets_df = pd.DataFrame(scores_all_test_sets)\n","all_sets_df.to_csv('SB31_cnn_test.csv')"],"metadata":{"id":"Dt9jDs1YUzaE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"swpHzPXJqPlk"},"outputs":[],"source":["#st1 = ['Ta', 'DY', 'SR', 'HR','Tw']\n","st20 = ['Ta', 'DY', 'SR', 'pp','HR', 'Wind', 'Tw_59']\n","st21 = ['Ta', 'SF', 'SR', 'pp', 'HR', 'Wind', 'Tw_59']\n","\n","set_n = [st20, st21]\n","\n","st_d20 = data[st20]\n","st_d21 = data[st21]\n","\n","sets = [st_d20, st_d21]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKqQyGuBTg-q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652501771922,"user_tz":420,"elapsed":1542857,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"}},"outputId":"f2b175d8-a4da-49cc-d8f8-87062e38dd75"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mModel M1......['Ta', 'Flow']\u001b[0m\n","Data dimension (Xn), (yn): (5850, 2) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 2) (1755, 2) (4095,) (1755,)\n","Sim 1\n","Epoch 164: early stopping\n","Loss:    1.8376686573028564\n","Val_loss: 1.911328673362732\n","\n","Sim 2\n","Epoch 218: early stopping\n","Loss:    1.8203673362731934\n","Val_loss: 1.9008475542068481\n","\n","Sim 3\n","Epoch 126: early stopping\n","Loss:    1.8375784158706665\n","Val_loss: 1.9079190492630005\n","\n","Sim 4\n","Epoch 125: early stopping\n","Loss:    1.8425848484039307\n","Val_loss: 1.9177422523498535\n","\n","Sim 5\n","Epoch 195: early stopping\n","Loss:    1.8223576545715332\n","Val_loss: 1.89572274684906\n","\n","Note: loss is MSE\n","Average of 5 repetitions: 1.381 RMSE_avge (+/- 0.003)\n","\n","\u001b[31mModel M2......['Ta', 'DY']\u001b[0m\n","Data dimension (Xn), (yn): (5850, 2) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 2) (1755, 2) (4095,) (1755,)\n","Sim 1\n","Epoch 355: early stopping\n","Loss:    1.697168231010437\n","Val_loss: 1.672511100769043\n","\n","Sim 2\n","Epoch 307: early stopping\n","Loss:    1.71004056930542\n","Val_loss: 1.6841559410095215\n","\n","Sim 3\n","Epoch 239: early stopping\n","Loss:    1.7116992473602295\n","Val_loss: 1.6821222305297852\n","\n","Sim 4\n","Epoch 197: early stopping\n","Loss:    1.7294093370437622\n","Val_loss: 1.7036287784576416\n","\n","Sim 5\n","Epoch 170: early stopping\n","Loss:    1.7348682880401611\n","Val_loss: 1.68833327293396\n","\n","Note: loss is MSE\n","Average of 5 repetitions: 1.299 RMSE_avge (+/- 0.004)\n","\n","\u001b[31mModel M3......['Ta', 'SF']\u001b[0m\n","Data dimension (Xn), (yn): (5850, 2) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 2) (1755, 2) (4095,) (1755,)\n","Sim 1\n","Epoch 172: early stopping\n","Loss:    1.7817119359970093\n","Val_loss: 1.7171887159347534\n","\n","Sim 2\n","Epoch 141: early stopping\n","Loss:    1.8103601932525635\n","Val_loss: 1.7345032691955566\n","\n","Sim 3\n","Epoch 278: early stopping\n","Loss:    1.7768903970718384\n","Val_loss: 1.728271722793579\n","\n","Sim 4\n","Epoch 313: early stopping\n","Loss:    1.7633520364761353\n","Val_loss: 1.7130330801010132\n","\n","Sim 5\n","Epoch 205: early stopping\n","Loss:    1.7888612747192383\n","Val_loss: 1.8210290670394897\n","\n","Note: loss is MSE\n","Average of 5 repetitions: 1.320 RMSE_avge (+/- 0.015)\n","\n","\u001b[31mModel M4......['Ta', 'SR']\u001b[0m\n","Data dimension (Xn), (yn): (5850, 2) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 2) (1755, 2) (4095,) (1755,)\n","Sim 1\n","Epoch 274: early stopping\n","Loss:    2.038404941558838\n","Val_loss: 2.0170071125030518\n","\n","Sim 2\n","Epoch 224: early stopping\n","Loss:    2.0272483825683594\n","Val_loss: 2.0132739543914795\n","\n","Sim 3\n","Epoch 159: early stopping\n","Loss:    2.0515530109405518\n","Val_loss: 2.027020215988159\n","\n","Sim 4\n","Epoch 207: early stopping\n","Loss:    2.037952184677124\n","Val_loss: 2.008986473083496\n","\n","Sim 5\n","Epoch 158: early stopping\n","Loss:    2.029947280883789\n","Val_loss: 2.0411558151245117\n","\n","Note: loss is MSE\n","Average of 5 repetitions: 1.422 RMSE_avge (+/- 0.004)\n","\n","\u001b[31mModel M5......['Ta', 'pp']\u001b[0m\n","Data dimension (Xn), (yn): (5850, 2) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 2) (1755, 2) (4095,) (1755,)\n","Sim 1\n","Epoch 165: early stopping\n","Loss:    2.16512131690979\n","Val_loss: 2.1350624561309814\n","\n","Sim 2\n","Epoch 134: early stopping\n","Loss:    2.1837422847747803\n","Val_loss: 2.136914014816284\n","\n","Sim 3\n","Epoch 162: early stopping\n","Loss:    2.171478033065796\n","Val_loss: 2.1394104957580566\n","\n","Sim 4\n","Epoch 118: early stopping\n","Loss:    2.17372989654541\n","Val_loss: 2.136277675628662\n","\n","Sim 5\n","Epoch 125: early stopping\n","Loss:    2.1816205978393555\n","Val_loss: 2.1343436241149902\n","\n","Note: loss is MSE\n","Average of 5 repetitions: 1.462 RMSE_avge (+/- 0.001)\n","\n","\u001b[31mModel M6......['Ta', 'HR']\u001b[0m\n","Data dimension (Xn), (yn): (5850, 2) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 2) (1755, 2) (4095,) (1755,)\n","Sim 1\n","Epoch 71: early stopping\n","Loss:    2.174852132797241\n","Val_loss: 2.1492793560028076\n","\n","Sim 2\n","Epoch 111: early stopping\n","Loss:    2.1628620624542236\n","Val_loss: 2.110340118408203\n","\n","Sim 3\n","Epoch 68: early stopping\n","Loss:    2.1954565048217773\n","Val_loss: 2.1341965198516846\n","\n","Sim 4\n","Epoch 55: early stopping\n","Loss:    2.176161050796509\n","Val_loss: 2.112506151199341\n","\n","Sim 5\n","Epoch 85: early stopping\n","Loss:    2.1614108085632324\n","Val_loss: 2.1186368465423584\n","\n","Note: loss is MSE\n","Average of 5 repetitions: 1.458 RMSE_avge (+/- 0.005)\n","\n","\u001b[31mModel M7......['Ta', 'Wind']\u001b[0m\n","Data dimension (Xn), (yn): (5850, 2) (5850,)\n","Training and test data dimensions: \n"," train_x, train_y, test_x, test_y: \n"," (4095, 2) (1755, 2) (4095,) (1755,)\n","Sim 1\n","Epoch 151: early stopping\n","Loss:    2.164926290512085\n","Val_loss: 2.153268575668335\n","\n","Sim 2\n","Epoch 173: early stopping\n","Loss:    2.149379014968872\n","Val_loss: 2.0875425338745117\n","\n","Sim 3\n","Epoch 142: early stopping\n","Loss:    2.153496742248535\n","Val_loss: 2.0934836864471436\n","\n","Sim 4\n","Epoch 102: early stopping\n","Loss:    2.164288282394409\n","Val_loss: 2.131537675857544\n","\n","Sim 5\n","Epoch 219: early stopping\n","Loss:    2.147214651107788\n","Val_loss: 2.0863242149353027\n","\n","Note: loss is MSE\n","Average of 5 repetitions: 1.453 RMSE_avge (+/- 0.009)\n","\n","[[1.3498362282915335, 1.3430956644624035, 1.3481118137102035, 1.3527520738457832, 1.3441826766957152], [1.2986593265841693, 1.3024567132299048, 1.3038830132810988, 1.313334154541869, 1.3104610500289138], [1.3282614245189968, 1.3352476871044052, 1.3295183390184266, 1.3263984228238668, 1.360528872751697], [1.4235070053533847, 1.4205696066291305, 1.4282593812051823, 1.4208702477873774, 1.4303143070502073], [1.4654657696279685, 1.4699208077359183, 1.4674841112774635, 1.4673391018057518, 1.4673863837793875], [1.4787880408455853, 1.4639177574497764, 1.4734633280653586, 1.46543079057398, 1.4671016127662722], [1.4830279210884005, 1.4605159049207959, 1.4623903608388442, 1.4777260691615806, 1.4585233927810102]]\n","[[1.3825081096914882, 1.3787122379226053, 1.381274429381432, 1.3848257119037952, 1.3768525215353857], [1.2932559613470773, 1.2977502466156354, 1.2969665035460614, 1.305231220910327, 1.299358701250498], [1.3104155200332614, 1.317005372041537, 1.3146374872159925, 1.3088289419593009, 1.3494551442151679], [1.4202136719819567, 1.4188988527698088, 1.4237346016684989, 1.417387283526304, 1.4286901611986877], [1.4611852099280236, 1.4618187352802277, 1.4626723815530451, 1.4616012289492029, 1.4609392951505515], [1.4660419903891664, 1.4527009735001224, 1.460888866900253, 1.4534462194318583, 1.4555536299653133], [1.4674020628603852, 1.444833047059248, 1.44688766836466, 1.4599786559595809, 1.444411373167389]]\n"]}],"source":["from termcolor import colored\n","from sklearn.model_selection import train_test_split\n","data = raw_d.drop(['scenario'],axis=1)\n","\n","#sets = [set1,set2,set3,set4,set5,set6,set7]\n","labels=[str(r) for r in sets]\n","sets_names = [f'M{i}' for i in range(1, 12)]\n","#print(\"Models to be tested: \", sets_names)\n","\n","names = set_n\n","\n","scores_all_train_sets = []\n","scores_all_test_sets = []\n","ccc = 0\n","for sett in sets:\n","  #print (\"Model M\"+str(ccc+1)+\"......\")\n","  print (colored(\"Model M\"+str(ccc+1)+\"......\"+str(names[ccc][:-1]), 'red'))\n","  sett = np.asarray(sett, dtype=np.float32) # Convert pandas df to np array\n","\n","  ### split into Predictor and Response variables\n","  Xn = sett[:, :-1]\n","  yn = sett[:, -1]\n","  print (\"Data dimension (Xn), (yn):\", Xn.shape, yn.shape)\n","\n","  # Split into train and test (Random split)\n","\n","  # Test-size: 30%, Train-size: 70%\n","  train_xn, test_xn, train_yn, test_yn = train_test_split(Xn, yn, test_size=0.3, random_state=1)\n","  print(\"Training and test data dimensions: \\n train_x, train_y, test_x, test_y: \\n\",\n","        train_xn.shape, test_xn.shape, train_yn.shape, test_yn.shape)\n","\n","  n_epochs = 1500   #  <-----  you might SET\n","  batch_size = 100\n","\n","  all_loss, all_val_loss = [],[]\n","  all_preds, all_scores_train, all_scores_test = [], [], []\n","\n","  n_repeats = 5  #     <----------------------- SET\n","  for i in range(n_repeats):\n","    print (\"Sim\", i+1)\n","    b_model = best_nn_model(len(train_xn[0]))   #  -----> call model\n","    early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1, mode='auto') #We will wait 'patience=10' epochs before training is stopped\n","    hist_model = b_model.fit(train_xn, train_yn,validation_data=(test_xn, test_yn),\n","                          epochs=n_epochs, batch_size=batch_size, verbose=0, callbacks=[early_stop])\n","    #print(hist_model.history['loss']) # Get list of 'loss' at each epoch\n","    all_loss.append(hist_model.history['loss']) # For other error reports see: https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n","    all_val_loss.append(hist_model.history['val_loss']) # This loss is from the 'test' data\n","\n","    print (\"Loss:   \", hist_model.history['loss'][-1])\n","    print (\"Val_loss:\", hist_model.history['val_loss'][-1])\n","\n","\n","    ### Prediction\n","    ### Prediction on train\n","    pred_yn_train = b_model.predict(train_xn, verbose=0)\n","    pred_yn_train_flat = [item for sublist in pred_yn_train.tolist() for item in sublist]\n","    #print (\"Prediction on train data (Sim\",j+1, \") \\n\", pred_y_train_flat)\n","    all_preds.append(pred_yn_train_flat)\n","    #pred_y_test_flat.to_csv('predict.csv')  # save updated dataset\n","\n","    # Prediction Error. Root mean squared error or RMSE\n","    error_train = sqrt(mean_squared_error(train_yn, pred_yn_train))#error = measure_rmse(test, predictions)\n","    #print(\"RMSE train (sim\",j+1,\"):\",'  %.3f' % error_train) #E:\n","\n","    all_scores_train.append(error_train) #all_scores.append(scores)\n","    #print (all_scores)\n","\n","    ### Prediction on test\n","    pred_yn_test = b_model.predict(test_xn, verbose=0)\n","    pred_yn_test_flat = [item for sublist in pred_yn_test.tolist() for item in sublist]\n","    #print (\"Prediction on test data (Sim\",j+1, \") \\n\", pred_y_test_flat)\n","    all_preds.append(pred_yn_test_flat)\n","    #pred_y_test_flat.to_csv('predict.csv')  # save updated dataset\n","\n","    # Prediction Error. Root mean squared error or RMSE\n","    error_test = sqrt(mean_squared_error(test_yn, pred_yn_test))#error = measure_rmse(test, predictions)\n","    #print(\"RMSE test(sim\",j+1,\"):\",'   %.3f' % error_test) #E:\n","\n","    all_scores_test.append(error_test) #all_scores.append(scores)\n","    #print (all_scores)\n","    print ()\n","\n","  #print (\"Tested variables (Header):\", var_tested) # print column names\n","  # summarize and plot scores\n","  print (\"Note: loss is MSE\")\n","  scores_m, score_std = mean(all_scores_test), std(all_scores_test)\n","  print('%s: %.3f RMSE_avge (+/- %.3f)' % ('Average of '+str(n_repeats)+\" repetitions\", scores_m, score_std))\n","\n","\n","  #all_sets.append(all_scores_test) #Store scores of all models (M1, M2,...) as list od lists\n","  scores_all_train_sets.append(all_scores_train) #Store scores of all models (M1, M2,...) as list od lists\n","  scores_all_test_sets.append(all_scores_test) #Store scores of all models (M1, M2,...) as list od lists\n","\n","  print ()\n","  ccc += 1\n","\n","  if ccc%10 == 0:\n","    print (\"Preliminar print\")\n","    print (scores_all_train_sets)\n","    print (scores_all_test_sets)\n","\n","print (scores_all_train_sets)\n","print (scores_all_test_sets)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vINsqQt2-9uz"},"outputs":[],"source":["from google.colab import files\n","import pandas as pd\n","all_sets_df = pd.DataFrame(scores_all_train_sets)\n","all_sets_df.to_csv('SB31_cnn_train.csv')\n","all_sets_df = pd.DataFrame(scores_all_test_sets)\n","all_sets_df.to_csv('SB31_cnn_test.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5r2zz--SXCFH"},"outputs":[],"source":["### Plot of results\n","import matplotlib.pyplot as plt\n","scores_all_models = all_sets # dd1 #\n","#sets_names = ['M-32-21', 'M-256-21','M-256-31','M-32-31','M-128-31'] # It is computed above\n","### Plot performance of all the models\n","fig = plt.figure(figsize=(8, 4), dpi=80)\n","plt.boxplot(scores_all_models, labels=sets_names, showmeans=1,\n","            meanprops={\"marker\":\"s\",\"markersize\":\"4\",\"markerfacecolor\":\"white\",\n","                       \"markeredgecolor\":\"blue\", \"markeredgewidth\":\"2\"})\n","\n","plt.xlabel(\"CNN Model\")\n","plt.ylabel(\"RMSE\")\n","plt.grid()\n","plt.show()\n","\n","fig.savefig('cnn_sb31.png', dpi=540)"]},{"cell_type":"markdown","source":["### Resources to print as pdf"],"metadata":{"id":"l-Ntg_y0ihFR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"N6KKcND1vwHt"},"outputs":[],"source":["!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n","!pip install pypandoc\n","!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10654,"status":"ok","timestamp":1650353981148,"user":{"displayName":"Efrain Noa Yarasca","userId":"13064186780466105567"},"user_tz":420},"id":"-BMldYXEwxJE","outputId":"764368c5-2127-49bf-8dd8-45aeec7459e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["[NbConvertApp] Converting notebook /content/drive/MyDrive/Colab_Notebooks/MLP_models_for_classes/011_cnn_sb31_submit2.ipynb to pdf\n","[NbConvertApp] Support files will be in 011_cnn_sb31_submit2_files/\n","[NbConvertApp] Making directory ./011_cnn_sb31_submit2_files\n","[NbConvertApp] Writing 77222 bytes to ./notebook.tex\n","[NbConvertApp] Building PDF\n","[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n","[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n","[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n","[NbConvertApp] PDF successfully created\n","[NbConvertApp] Writing 165716 bytes to /content/drive/MyDrive/Colab_Notebooks/MLP_models_for_classes/011_cnn_sb31_submit2.pdf\n"]}],"source":["!jupyter nbconvert --to pdf /content/drive/MyDrive/Colab_Notebooks/MLP_models_for_classes/011_cnn_sb31_submit2.ipynb"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}